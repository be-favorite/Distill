[
  {
    "path": "posts/2022-09-12-statistics-playbook-1/",
    "title": "슬기로운 통계생활: #1 통계학 전공자의 대학원 진로 고민",
    "description": "취업 준비와 대학원 진학의 기로에서 고민하고 깨달은 것들",
    "author": [
      {
        "name": "Taemo Bang",
        "url": {}
      }
    ],
    "date": "2022-09-13",
    "categories": [
      "Statistics Playbook"
    ],
    "contents": "\n\nContents\n진로에 대한\n고민\n호기심을 불러일으킨 두 가지\n키워드\n참담한 취업 실패에서 배운\n것들\n대학원에 들어가며 다짐했던\n것\n맺음말\n\n\n유튜브 채널\n슬기로운 통계생활에서 운영하는 블로그에 기고했던 칼럼들을\n최신화하여 다시 적어보려고 합니다. 첫 번째 칼럼 주제는 대학원에\n대한 고민입니다. 저는 늘 고민과 생각이 많은 사람인데요.😂 때는\n제가 통계학과 학부 4학년이던 2018년으로 거슬러 올라갑니다. 4학년 1학기\n때는 학내 교환학생 프로그램에 신청하여 한 학기를 영국의 쉐필드대학(The\nUniversity of Sheffield)에서 보내게 됩니다.\n\n🔗과거에\n기고했던 칼럼\n\n아쉽게도 이 곳에서 통계학 전공 과목을 들을 기회는 없었습니다. 영어와\n여러 가지 교양 과목을 수강했고, 시간이 많았던 때라 실컷 놀면서 자연스레\n진로에 대한 고민을 다시 한 번 깊게 해보게 되었습니다.\n진로에 대한 고민\n당시 저는 통계학 전공을 살려 Data Scientist라 표현되는 직업을 갖고\n싶었습니다. 그래서, 다음과 같은 두 가지 선택지에서 고민하기\n시작했습니다.\n취업 준비\n통계학 대학원 진학\n어중이떠중이 기질이 있었던 저는 깊은 고민 끝에 2년이라는 시간을\n통계학 대학원에 투자할 용기가 없어, 4학년 1학기를 쉐필드에서 마치고\n한국으로 돌아가 취업 준비를 해보기로 결심했습니다. 학부 졸업 요건과\n취업에 필요한 기본 요건1은 준비가 되어있었고, 무엇보다 마음\n속에 지금 상태로 취업 준비를 해봐도 되겠다는 알 수 없는 자신감이\n있었습니다. 그 이유는 지금 생각해보면 정말 별것 아닌 것들 때문이었죠.\n기본적인 소양에 불과한 평균 평점(3.93/4.5)과 전공 평균 평점(4.1/4.5),\n그리고 지금 다시 돌아보면 정말 형편없었던 R 숙련도에 대한 자부심은 제게\n“이정도면 취업 준비를 해봐도 되지 않을까?” 하는 근거없는 자신감을 갖게\n했죠. 이렇게 취업 준비를 결심하고 채용 공고를 들여다보면서 시간을 보내는\n와중에, 계속해서 눈에 밟히던 두 가지 키워드가 있었습니다.\n호기심을 불러일으킨 두 가지\n키워드\n2016년 구글 딥마인드 팀이 개발한 바둑 AI 알파고가 이세돌과의 바둑\n대국에서 압도적으로 승리를 거두며, 수십 년에 걸쳐 발전해온 딥러닝이라는\n기술은 마치 최근 개발된 혁신적인 신기술인냥 세상의 주목을 받게\n되었습니다. 매스컴의 주목에 따라 뉴스에서 종종 등장하던 두 단어\n“머신러닝”과 “딥러닝”은 제게 또다른 호기심을 심어주었습니다.\n\n당시 학부 전공 과목으로 데이터마이닝을 수강한 상태였던터라 이러한\n호기심 매우 자연스러운 현상이었던 것 같습니다. 그러나, 당시 통계학 학부\n4학년에 불과하던 제게 머신러닝, 딥러닝과 같은 키워드는 머릿속에 큰\n그림은 커녕 기존에 배웠던 전공 과목2들과 자연스러운 비교를\n하면서 혼란을 가중시킬 뿐이였죠. 내가 배웠던 것들과 두 키워드는 어떤\n관련이 있는지 알고 싶었고, 심지어는 “전자와 후자 중 어떤 것이 더 나은\n방법론인가?” 와 같이 지금 생각해보면 참 바보 같은 생각을 했었습니다.\n이러한 생각들은 Data Scientist의 꿈이 있었던 사람에게 왠지 모를 두려움과\n불안감을 심어주었습니다. 그래서, 이것 저것 찾아보며 두 기술에 대해\n이해해보려고 노력했습니다. 이 과정에서 문득 “과연 내가 이 상태로 실무에\n나가서 호기심이 있는 기술, 또는 직무에 꼭 필요로 되는 기술이 있을 때\n이러한 기술들을 독학하여 실무에 적용할 수 있을까?” 하는 생각을 했습니다.\n4학년 2학기 졸업예정자 신분으로서 Data Scientist 직무로의 취업을\n성공한다고 한들, 직무를 잘 수행해내며 스스로 발전할 수 있을지에 대한\n의구심이 생겼죠. 그래서, 마음속에서는 대학원 진학에 대한 열망이 다시 한\n번 피어오르고 있었습니다.\n대학원 진학이라는 길이 머릿속을 떠나지 않았습니다. 그래서, 4학년\n2학기 딱 한 학기만 학부 졸업 예정자로서 취업 준비를 하며 제가 다니던\n본교의 통계학 대학원 진학 준비를 병행해서 해보기로 했습니다. 당연히 취업\n준비 결과는 참담했습니다:\n(학부 4학년 2학기) 2018년 하반기 채용\n지원 결과그때 제 수준을 지금 생각해보면 이러한 결과는 당연했다는 생각이\n드네요.😂\n참담한 취업 실패에서 배운\n것들\n취업 준비 결과 실질적으로 손에 쥔 것은 없었지만, 수많은\n채용공고를 보고 자기소개서를 쓰며 얻은 것과 배운 것들은\n많았습니다:\n1 2018년까지 내가 해온 활동에\n대한 정리\n대학원 졸업을 앞두고 내가 해온 활동에 대한 정리를 시작했다면, 졸업\n논문 작업과 겹쳐 취업 준비에 매우 어려움을 겪었을거라 생각합니다.\n2 자기소개서를 쓰는 방식\n당시 썼던 자기소개서들을 올해 이직 준비를하며 썼던 자기소개서들과\n비교해보면, 과거의 제가 썼던 자기소개서는 정말 형편없었습니다. 그러나,\n첫 자기소개서를 대학원을 졸업하던 시기에 쓰기 시작했다면 그야말로\n아찔하네요.\n3\n우리나라 기업에서 Data Scientist/Analyst 채용시 원하는 구체적인\n역량\n당시 완벽하게 깨우치지는 못했지만 수많은 채용공고를 들여다보니\n준비해야할 방향이 조금이나마 보였던 것 같습니다. 취업이나 이직을\n준비하시는 분들이 아니더라도 업계의 인재 영입 동향 파악을 위해 틈틈히\n채용공고를 들여다보시는 것을 추천드립니다. 이번에 이직 준비를 하서면서도\n우리나라 기업에서 낸 수많은 Data Scientist/Analyst 채용 공고를\n들여다보았는데, 우리나라의 분석 직군들의 직무들도 점차 세분화 되어 잘\n정립되어 가고 있다는 느낌을 받을 수 있었습니다. 물론, 여전히 채용 공고를\n아무리 읽어 봐도 무슨 일을 하게 될 지 알 수 없는 그런 공고들도 종종\n보였으나, 이건 어느 직무에서든 종종 보이는 성의없게 쓰여진\n채용공고이므로 별 의미를 두지 않았습니다. 세분화되어 잘 정립되어 가고\n있는 우리나라 분석 직군의 세부 직무들을 자세하게 알아보고 싶은 분들께는\n변성윤님이 올려주신 🔗유튜브 영상을\n추천드립니다.\n4 석사학위에 대한 필요성\n제 머릿 속에 대학원 진학이라는 키워드가 계속해서 맴돌았기 때문일지도\n모르겠습니다. 2018년 당시 석박사 채용을 통해서만 Data Scientist 직무를\n뽑는 경우도 종종있어 지원조차 못하는 기업들이 있었고3,\n4년제 대졸 신입사원 채용으로 뽑더라도 우대사항에는 늘 석사학위 보유자\n키워드가 함께 자리하고 있었습니다. Data Scientist/Analyst 채용 시 통계학\n학사와 석사가 경쟁하면 기업 입장에서는 기본적으로 어떤 지원자가 더\n매력적이겠습니까? 학위를 뛰어넘을만한 좋은 경력이나 포트폴리오를 갖고\n있지 않는 이상 석사 학위 보유자를 선호할 것입니다. 단, 학위 자체가 어떤\n특정한 어드벤티지를 준다고는 생각하지 않았습니다. 그만큼 석사 학위\n보유자라는 책임감을 가져야하만하고 기업의 기대에 맞는 수준을 갖는 사람이\n되어야만 한다고 생각했죠. 말 그대로 학위는 우리를 둘러싼 껍질에 불과한\n기본 아이템이라고 표현하면 적절할까요? 그러나, 당시 학사 학위와 빈약한\n포트폴리오를 갖고있던 제게 석사 학위를 뛰어넘을만한 Data Scientist\n직무로의 취업 준비 방법은 떠오르지 않았죠. 그래서, 대학원 진학을 결정한\n것이고요.\n이렇게 졸업 예정자로서 취업 준비를 한 번 해봤던 경험은 제게 통계학\n대학원 진학에 대한 필요성을 직접 피부로 느끼게 해주었습니다. 대학원에\n진학하여 열심히 공부할 수 있었던 동기부여 또한 마음 속 깊히 채워넣을 수\n있었습니다. 그 결정을 한 당시를 돌아보면 통계학 대학원 진학에 대한\n후회는 전혀 느껴지지 않습니다. 제 인생에 정말 탁월한 결정이였죠. 오히려\n취업 준비를 하지 않고 통계학 대학원 준비에 올인했다면 더 좋은 결과를\n가져올 수 있었을까? 하는 무의미한 생각을 하곤 합니다.😂 당시의 저처럼\n현재 통계학 대학원 진학에 대한 고민을 품고 있는 학부생들의 선택은 당연히\n본인의 몫입니다. 다만, 열심히 공부함과 동시에 자신을 부지런히\n브랜딩한다는 가정 하에, 통계학 대학원 진학은 Data Scientist/Analyst로의\n취업에 무조건 플러스가 될 것이라고 말씀드리고 싶네요.😀 이 말에도 대학원\n진학에 확신이 서질 않는다면, 자신의 수준을 한 번 냉정하게 바라보시고\n실무에 나갈 준비가 되었는지 본인에게 질문을 던져보시기 바랍니다. 질문의\n답이 “Yes”라면 취업 준비를 해보시는 것 또한 정말 좋은 경험이\n되실겁니다.\n대학원에 들어가며 다짐했던\n것\n저는 당시 Data Scientist/Analyst 직무로의 취업을 꿈꿨지만, 어느 기업\n또는 어느 업계로 가고 싶다는 구체화는 전혀 되어있지 않던 상태였습니다.\n그래서, 수많은 분야에서 의사결정의 도구로 사용되고 있는 Data\nScience/Analytics의 특성상 어떤 식으로 커리어 방향을 잡아 나갈지, 무엇을\n공부해야 할지 참 막막했습니다. 이러한 혼란 속에서 다짐했던 것은 2가지\n였습니다. 대학원을 졸업할 무렵에는 누구에게나 자신있다고 말할 수 있는\n분석 언어 1가지, 분석 분야 1가지를 만들겠다고 말이죠.4\n대학원을 졸업하던 당시 가장 자신있었던 분석 언어와 분석 분야는 R과\n시계열 자료분석 이었습니다. 이 두 가지 무기로 첫 번째 직장에 취업을 하고\n머릿 속에 그리던 직무를 수행할 수 있었죠. 2가지 다짐의 개인적 근거는\n이쪽 업계는 이것저것 두루두루 잘하는 Generalist 보단 하나 혹은 두 가지를\n특출나게 잘하는 Specialist를 선호한다고 생각했기 때문입니다. 다양한\n백그라운드를 가진 사람들이 일하는 Data Science/Analytics 업계인 만큼,\n두리뭉술한 사람 보다는 확실한 아이덴티티가 있는 사람이 채용시장에서 높은\n선호도를 보일 것이라고 생각했죠. 이쪽 업계에 있을수록 Generalist가\n되기란 참 어렵지 않나 하는 생각을 합니다. Generalist가 되려다 이것저것\n얕게 알고있는 특색없는 Generalist가 될 수 있다는 것을 유념하시기\n바랍니다.\n반대로, Data Scientist/Analyst 직무로의 취업을 꿈꾸며 어느 기업 또는\n어느 업계로 갈지에 대한 구체화가 끝나신 분들도 있을 수 있겠죠. 이 분들은\n참 똑똑한 분들이라 생각합니다. 이러면 취업 준비가 꽤 편해지니까요. 가고\n싶은 기업의 링크드인, 기술 블로그 등을 팔로우 하고 채용 공고를 미리미리\n들여다보며, 자신이 해당 포지션으로 가기 위해 배워야할 것들을 구체화할 수\n있습니다. 그럼, 자연스레 해당 기업에서 원하는 Specialist가 되는 길을\n걷게 되겠죠. 공부 외에도 적극적인 액션을 취해보시기를 권합니다.\nSpecialist가 되기 위해 공부해야하는 분야에 커뮤니티가 있다면 가입해서\n활동도 해보고, 링크드인에 자신이 가고자 하는 업계 또는 기업에서 Data\nScience/Analytics를 수행하고 있는 분들이 보인다면 콜드메일(메시지)을\n보내보기도 하면서요.😀\n맺음말\n2021년 2월에 통계학 석사학위를 마치고, Data Scientist 직무로 현업에\n있는 사람으로서, 2018년의 저와 비슷한 고민을 하고 있는 분들께 하고 싶은\n몇 마디를 하고 글을 마치려고합니다.\n대학원 졸업을 앞두고 제가 성장한 부분 중 가장 뜻 깊게 생각되는 부분은\n특정 알고리즘에 관한 이해가 아닌, 앞으로도 쏟아져 나올 분석 방법론,\n그리고 소프트웨어 역량이라 할 수 있는 R의 수많은 패키지 등을 혼자\n공부하고 정리할 튼튼한 발판을 마련했다는 점이었습니다. 그리고, 무엇보다\n나를 브랜딩하고 PR 할만한 장치들도(e.g. Github, 개인 블로그) 갖출 수\n있었죠. 제가 학부를 졸업하고 바로 해당 직무로 취업을 했다고 한들 이렇게\n튼튼한 발판과 자신을 브랜딩하는 나만의 방법이 없이는 언젠가 성장의\n한계에 마주했을 거라고 생각합니다.\n그래서, 만약 본인이 Data Scientist/Analyst 직무에 대해 열정과\n호기심이 있는 통계학 전공자라면 주저하지 마시고 대학원에 진학하시는 것을\n추천합니다. 호기심이 이끄는대로 열심히 이것저것 찾아보며 공부하고,\n자신을 가꿔나갈 각오가 되어있는 분들께 대학원은 이쪽 업계에서 무조건\n플러스라고 생각하니까요. 물론, 꼭 통계학 대학원이 아니여도 상관없습니다.\n본인이 원하는 직무와 좀 더 관련성있는 연구실을 운영 중인 다른 학과가\n있다면 해당 학과로의 진학을 추천드립니다. 예를 들어, 만약 본인이 특히\n머신러닝이나 딥러닝 쪽 연구를 통해 예측 모델링을 전문적으로 수행하는\n사람이 되고 싶다면 통계학 대학원이 아닌 컴퓨터 과학(Computer Science,\n또는 소프트웨어 학과라 일컫는) 쪽에서 해당 분야를 전문적으로 연구하시는\n교수님의 연구실에 들어가는 것을 추천드리고싶습니다.5\n이런저런 이야기들을 하다보니 글이 꽤 길어졌습니다. 제가 느낀 것들을\n바탕으로 쓴 글이니 정답이라고 생각하진 않으셨으면 합니다. 취업, 그리고\n진로에 대한 길을 만들어 나가는 것에는 수많은 정답이 존재하니까요.\n\nTOEIC, OPIC 등 채용 지원을 위해\n필요한 기본적인 것들↩︎\n회귀분석, 통계적 가설검정, 실험설계,\n시계열 자료분석 등↩︎\n이건 이직을 준비하는 2022년에도\n마찬가지였습니다. 그러나, 2022년의 저는 석사학위를 보유하고 있었죠. 예를\n들어, 신한은행의 경우 (정규직) 데이터분석 직군은 석박사채용으로만\n하고있는 것으로 알고있습니다. 그외 은행은 데이터분석 직군은 전문계약직\n형태로 뽑는 것으로 알고 있습니다..😭↩︎\n여기서 “자신있다”의 의미는 해당 분석\n언어와 분석 분야를 다 알고 있다는 것이 아닌, 모르는 것을 마주치더라도\n독학하여 다 내 것으로 만들 수 있는 자신이 있다는 것을 의미합니다.↩︎\n혹시, 통계학 대학원에서 머신러닝 또는\n딥러닝에 기반한 예측 모델링 기법을 깊게 연구하는 연구실이 있다면, 댓글로\n공유해주시면 많은 분들에게 도움이 될 것 같습니다.↩︎\n",
    "preview": "posts/2022-09-12-statistics-playbook-1/preview.jpg",
    "last_modified": "2022-09-19T22:37:31+09:00",
    "input_file": "statistics-playbook-1.knit.md"
  },
  {
    "path": "posts/2022-08-07-coloring-guide-for-ggplot2/",
    "title": "ggplot2 컬러링 가이드",
    "description": "더 적은 수의 컬러로 직관적인 시각화를 수행하는 방법",
    "author": [
      {
        "name": "Taemo Bang",
        "url": {}
      }
    ],
    "date": "2022-08-14",
    "categories": [
      "Visualization",
      "R"
    ],
    "contents": "\n\nContents\n준비하기\n음영을\n활용하자\n음영만으로는 부족해\n전달하고 싶은 내용만\n강조하자\n라벨링\n활용하기\n맺음 말\n\n\n오늘은 ggplot2에서 더 적은 수의 컬러로 더 직관적인\n시각화를 가능하게끔 해주는 4가지 방식에 대해 소개해보려고 합니다.\nggplot2를 바탕으로 진행되는 예제이긴 하나, 본 글에서 소개할\n방식들에 담겨있는 아이디어는 언어, 시각화 라이브러리를 막론하고 적용이\n가능할거라고 봅니다.물론, ggplot2만큼 짧고 가독성 좋은\n코드로 구현이 가능할지는 미지수이지만요.😁 본 글에서\nggplot2의 그래프 문법 1이 갖는 강력한 힘을\n확인하실 수 있을 겁니다.\n\n본 글은 (Rapp 2022)를\n기반으로 작성되었습니다.\n대중들은 말이 아닌 그림을 기억합니다. 그래서, 잘 만들어진 데이터\n시각화는 강력한 힘을 갖습니다. 종종 데이터 시각화를 하시다가 지나치게\n많은 색을 사용하게 되어 오히려 전달력이 떨어진다는 느낌을 받은 적이 있지\n않으신가요? 그렇다면 이 글이 도움이 되실 수도 있겠습니다.😀\n준비하기\n본격적인 시작 전 몇 가지 준비를 하고자 합니다.\nggplot2에서 제공하는 다양한 테마 중\ntheme_minimal()을 사용할 예정이고, 폰트, 그림 제목과 색\n등에 몇 가지 조정을 줄 예정입니다:\nshowtext 패키지를 통해 Fira Sans font\n설정\n그림 제목은 기본 좌측 정렬, 색맹(color-blind)까지 고려한 Okabe Ito 컬러 팔레트\n사용\n본 글의 작성에 참고한 원 글의 저자는 Fundamentals of Data\nVisualization by Claus Wilke를 읽은 뒤, Okabe Ito 컬러 팔레트를\n선호하게 되었다고 합니다.\n\n\n\nlibrary(tidyverse)\nlibrary(showtext)\nfont_add_google(\"Fira Sans\", \"firasans\")\nshowtext_auto()\n\ntheme_customs <- theme(\n  text = element_text(family = 'firasans', size = 16),\n  plot.title.position = 'plot',\n  plot.title = element_text(\n    face = 'bold', \n    colour = thematic::okabe_ito(8)[6],\n    margin = margin(t = 2, r = 0, b = 7, l = 0, unit = \"mm\")\n  )\n)\n\ntheme_set(theme_minimal() + theme_customs)\n\n\n음영을 활용하자\nggplot2 패키지에서 제공하는 mpg 데이터셋을\n이용해 연도별 자동차 종류의 빈도를 시각화해봅시다. 전에 충분히\n보셨을만한 데이터라 생각해서, 데이터셋에 관한 설명은 스킵하겠습니다.\n이런 방식으로 시각화를 해보신 경험이 있으실겁니다:\n\n\nmpg |> \n  ggplot(aes(x = factor(year), fill = class)) +\n  geom_bar() +\n  labs(x = \"year\")\n\n\n\n자동차 종류가 많다보니 무려 7개의 컬러를 시각화에 사용하였습니다. 이\n그림이 틀렸다고 할 수는 없습니다. 다만, 좋은 시각화라고 할 수 있는지에\n대해 한 번 생각해보자는 겁니다. 제가 보기에 이렇게나 많은 수의 컬러를\n사용하는 시각화는 꽤나 정신없어 보인다고 느껴집니다.여러 수준을 갖는\n범주형 변수에 관한 컬러링에 있어서 더 적은, 최대 3개 정도의 컬러만\n사용하여 시각화를 수행하는 방법은 없을까요? 이제 그 아이디어를\n소개하고자 합니다. 자동차 종류를 구분하기 위해 색조(hues) 뿐만이 아닌,\n음영(shades)을 활용하는 것이죠. 3가지 컬러만을 사용해 투명도를 줌으로써\n7개의 자동차 종류를 구분해보겠습니다. 미리 말씀드리자면,\nminivan을 단독 하나의 그룹으로 설정해주어 이를 중심으로\n투명도가 줄어들고 늘어나게끔 만드는 것이 키 아이디어입니다.\n이를 위해서는 우선 데이터에 자동차의 종류를 3가지로 구분짓는 새로운\n그룹 변수를 생성해주어야 합니다:\n\n\n# Group classes into three groups (to reduce colors to 3)\ndat <- mpg |> \n  mutate(\n    year = factor(year),\n    class_group = case_when(\n      class %in% c('2seater', 'compact', 'midsize') ~ \"grp1\",\n      class == 'minivan' ~ \"grp2\",\n      T ~ \"grp3\"\n    )\n  )\n\n\n이를 바탕으로 우선 먼저 시각화를 해보죠:\n\n\nshades_plt <- dat |> \n  ggplot(aes(x = year, fill = class_group, alpha = class)) +\n  geom_bar() +\n  labs(\n    x = 'Year',\n    y = 'Counts',\n    alpha = 'Class',\n    title = 'Show shades, not hues'\n  )\nshades_plt\n\n\n\n색조와 음영까지 활용해 3가지 컬러정도로 줄이긴 했지만, 아직 전달력은\n매우 떨어집니다. 우리 눈으로 색조와 음영을 조합해 그림의 자동차 종류를\n구분해내는 것은 꽤 어렵죠. 투명도와 색상을 직접 조정해보겠습니다.\n투명도는 suv -> minivan까지 점차 줄어들고,\nminvan 이후부터는 다시 줄어든 양만큼 투명도가 늘어나도록\n설정을 해주려고 합니다:\n\n\n# Color-blind safe colors\ncolors <-  thematic::okabe_ito(3)\n# Possible levels of transparency (one for each class)\nalpha_max <- 1\nalpha_min <- 0.7\nalpha_vals <- c(\n  seq(alpha_max, alpha_min, length.out = 4), \n  seq(alpha_min, alpha_max, length.out = 4)[-1]\n)\nalpha_vals\n\n[1] 1.0 0.9 0.8 0.7 0.8 0.9 1.0\n\n\n\n# Tweak previous plot\nshades_plt <- shades_plt +\n  scale_fill_manual(values = colors) +\n  scale_alpha_manual(values = alpha_vals)\nshades_plt\n\n\n\n아까보다는 좀 낫습니다. 여기서 좀 더 개선을 해보자구요. 우측 범례를\n하나로 좀 통합해서 설정하면 좋을 것 같은데요. 7개로 구분되어 있는\n투명도에 컬러를 입혀줘서 말이죠. 꽤나 어려운 작업일 것 같지만,\nggplot2에서는 아주 손쉬운 작업입니다. guides()\n함수를 통해 가능합니다. fill에 관한\n범례(class_group)는 삭제를 한 뒤에,\nguide_legend()를 통해 alpha에 관한 범례에\nfill의 색상을 가져와 우리가 사전에 설정한 각각 3개\n그룹(class_group)의 컬러를 덮어씌워(override) 줄겁니다.\n말이 조금 복잡해보이지만, 코드를 보면 더 쉽게 이해하실 수 있습니다:\n\n\nshades_plt <- shades_plt +\n  guides(\n    fill = guide_none(),\n    alpha = guide_legend(\n      override.aes = list(fill = colors[c(1, 1, 1, 2, 3, 3, 3)]\n      )\n    )\n  ) \nshades_plt\n\n\n\n음영만으로는 부족해\n충분히 괜찮은 시각화를 했지만, 아쉬운 부분이 하나 있습니다. 인접한\n컬러 블록들에서는 자동차 종류 구분이 쪼~금 불편해 보입니다. 이 문제 또한\n손쉽게 해결해줄 수 있어요. 블록마다 선 하나씩만 그어주면 말이죠. 앞선\n시각화 코드에 geom_bar() 한 줄이면 해결할 수 있습니다. 이\n또한 그래프 문법의 힘이죠.😄\n\n\ndat |> \n  ggplot(aes(x = year, fill = class_group, alpha = class)) +\n  geom_bar(col = 'white') + # Add lines for distinction\n  scale_fill_manual(values = colors) +\n  scale_alpha_manual(values = alpha_vals) +\n  guides(\n    fill = guide_none(),\n    alpha = guide_legend(override.aes = list(fill = colors[c(1, 1, 1, 2, 3, 3, 3)]))\n  ) +\n  labs(\n    x = 'Year',\n    y = 'Counts',\n    alpha = 'Class',\n    title = 'Group categories together by color, \\nbut keep showing them'\n  )\n\n\n\n전달하고 싶은 내용만\n강조하자\n이제 조금 다른 이야기를 해보려고 합니다. 꼭 위와 같이 모든 범주에\n대해 컬러를 줄 필요는 없는 상황도 있지 않을까요? 예를 들자면, 우리가\n시각화를 통해 꼭 강조해서 전달하고 싶은 내용이 있을 때처럼요. 이번에\n사용할 예시 데이터는 흥미로운 다양한 데이터셋을 제공해주는 Our World in\nData에서 가져왔습니다. 미국인들을 대상으로 설문조사를 수행한\n자료인데요. 본 자료를 통해 우리가 알아보고자 하는 바는 “우리는 과연\n일생동안 누구와 시간을 많이 보내는가?”입니다. 여러 나이대의 미국인들을\n대상으로 하루에 평균적으로 누구와 얼마나 시간을 보내는지에 대해 조사한\n자료라고 할 수 있겠습니다. 위 링크의 차트에 아래 우측 탭을 보시면\nDownload를 눌러서 데이터를 받으실 수 있습니다:\n이 자료를 바탕으로 다음과 같은 그림을 그려볼 수 있습니다. 과연 나이에\n따라 우리가 시간을 함께 보내는 대상은 어떤 식의 패턴을 보이며\n변화할까요? 일반적으로는 다음과 같이 시각화를\n\n\n# Some data wrangling\ntime_data <- read_csv(\"./time-spent-with-relationships-by-age-us.csv\") |> \n  rename_with(\n    ~c('Entitity', 'Code', 'Age', 'alone', 'friends', 'children', 'parents', \n       'partner', 'coworkers')\n  ) |> \n  pivot_longer(\n    cols = alone:coworkers, \n    names_to = 'person',\n    values_to = 'minutes'\n  ) |> \n  janitor::clean_names() |> \n  filter(age <= 80)\n\n# Color-blind safe colors\ncolors <- thematic::okabe_ito(7)[-6]\n\n# Line plot\np <- time_data |> \n  ggplot(aes(x = age, y = minutes, col = person)) +\n  geom_line(size = 1.5) +\n  scale_color_manual(values = colors) +\n  coord_cartesian(xlim = c(15, 81), expand = F) +\n  scale_y_continuous(minor_breaks = NULL) +\n  labs(x = 'Age (in years)', y = 'Minutes', col = 'Time spent')\np\n\n\n\n보통 이렇게들 시각화하곤 하죠. 이런 종류의 그림은 스파게티\n플롯(spaghetti plot)이라고 표현하기도 합니다. 우리는 또 수많은 컬러에\n직면했습니다. 아울러, 이 그림 한 장만 놓고 봤을때는 무슨 말을 전달하고자\n하는지 파악하기가 참 힘듭니다. 제 눈엔 우선 2가지 인사이트가\n보입니다:\n우리는 일생동안 혼자서 가장 많은 시간을 보내게 된다.\n40대 즈음해서 아이와 보내는 시간은 줄어들면서, 혼자 보내는 시간이\n많아진다.\n이와 같이 만약 우리가 전달하고자 하는 인사이트가 확실한 상태라면,\n중요한 부분만 강조함으로써 이 지저분한 스파게티 플롯을 전달력 있는\n깔끔한 스파게티 플롯으로 만들어 줄 수 있습니다.\nggplothighlight 패키지가 그 해결책이 되어줍니다. 패키지\n안의 ggplothighlight() 함수를 이용해 레이어를 하나더\n얹어서, 필터링을 해줄 수 있어요. 아주 편리한 패키지죠. 특정 조건을\n만족하지 않는 데이터 포인트는 모조리 회색으로 표현이 됩니다. 먼저 첫\n번째 인사이트를 그림으로 표현해봅시다. 코드 1줄 정도만 추가해주면\n가능합니다.\n\n\nlibrary(gghighlight)\nalone_plt <- p + \n  gghighlight(person == 'alone', use_direct_label = F) +\n  labs(title = 'Emphasize just one or a few categories')\nalone_plt\n\n\n\n이 그림에 텍스트를 추가하여 우리가 하고싶은 이야기를 좀 더 강조할\n수도 있습니다:\n\n\nalone_plt +\n  annotate(\n    'text',\n    x = 15,\n    y = 455,\n    label = 'We spend a lot of time alone...',\n    hjust = 0,\n    vjust = 0,\n    family = 'firasans',\n    size = 7\n  )\n\n\n\n하고 싶은 이야기가 하나가 아니라 여러개라면 어떤 방법이 있을까요?\n전혀 문제가 되지 않습니다.😀 gghighlight()를 사용해 그저\n여러 조건 넣어주기만 하면 됩니다.\n\n\nage_40_plt <- p + \n  gghighlight(\n    person %in% c('alone', 'children'), \n    age >= 38, \n    use_direct_label = F\n  ) +\n  geom_segment(x = 38, xend = 38, y = -Inf, yend = 300, linetype = 2, col = 'grey20') +\n  labs(title = 'Emphasize just one or a few categories') \n\nage_40_plt +\n  annotate(\n    'text',\n    x = 15,\n    y = 403,\n    label = 'Around the age of 40, we spend \\nless time with children and \\nmore time alone.',\n    hjust = 0,\n    vjust = 0,\n    family = 'firasans',\n    lineheight = 0.85,\n    size = 5.5\n  )\n\n\n\n라벨링 활용하기\n앞서 본 모든 그림들에서는 그림의 이해를 돕기위한 범례(legend)가\n우측에 자리하고 있었습니다. 범례는 그림에서 꽤나 큰 공간을 차지합니다.\n아울러, 범례는 그림에 집중도를 떨어뜨릴 수 있죠. 그림의 이해를 위해서는\n필연적으로 범례와 그림을 번갈아가며 봐야하니까요. 이 문제를 해결할\n방법은 없을까요? 범례를 없애고 그림에 라벨링을 통해 우리가 하고자 하는\n이야기를 전달하면 어떨까요? 싱글 레이블에 대해서는\nannotate(), 다중 레이블에 대해서는\ngeom_text()를 이용해 라벨링을 하면 되는데요. 우선\nannotate()를 활용해 스파게티 플롯 예제를 개선시켜\n보겠습니다:\n\n\nalone_plt +\n  annotate(\n    'text',\n    x = 15,\n    y = 455,\n    label = 'We spend a lot of time alone...',\n    hjust = 0,\n    vjust = 0,\n    family = 'firasans',\n    size = 7\n  ) +\n  annotate(\n    'text', \n    x = 70, \n    y = 420, \n    label = 'alone',\n    hjust = 0,\n    vjust = 0,\n    size = 7,\n    family = 'firasans',\n    color = colors[1]\n  ) +\n  labs(title = 'Label directly') +\n  theme(legend.position = 'none')\n\n\n\n이러한 방식으로 공간도 세이브하고, 그림에 집중도도 훨씬 높혀줄 수\n있죠. 범례와 플롯을 눈으로 왔다갔다 할 필요도 없고요. 여기서 조금 더\n개선을 해볼까요? 현재 위 그림에는 alone이라는 단어가 중복으로\n들어가있죠. 강조한 선 아래 alone을 없애고 선과 동일한 색상을 좌측 문장의\nalone에 넣어주는 것은 어떨까요? 더 매력적인 시각화가 될 것만 같다는\n생각이 들지 않나요?\n이를 위해서는 ggtext 패키지를 활용해 HTML 문법을\n이용해야합니다. annotation()의 text geom을\nrichtext geom으로 바꾸고, 우리가 컬러를 반영하고자 하는\n텍스트에 대해서는 HTML 코드를 포함하는 문자열을 만들어 주는 과정이\n필요합니다. 말이 좀 복잡해보이지만, 코드는 꽤 간단합니다:\n\n\nlibrary(ggtext)\ncolor_alone <- glue::glue(\n  \"We spend a lot of time <span style = 'color:{colors[1]};'>alone<\/span>...\"\n)\ncolor_alone\n\nWe spend a lot of time <span style = 'color:#E69F00;'>alone<\/span>...\n\n\n\nalone_plt +\n  labs(title = 'Label directly') +\n  annotate(\n    'richtext',\n    x = 15,\n    y = 400,\n    label = color_alone,\n    hjust = 0,\n    vjust = 0,\n    family = 'firasans',\n    size = 6,\n    label.color = NA\n  ) +\n  theme(legend.position = 'none')\n\n\n\n멋지지 않습니까? HTML 문법에 익숙하지 않은 분들은 컬러링을 넣어주는\n형태를 기억하시기 바랍니다. 이런 식으로 직접적으로 라벨링 하는 방식은\n스파게티 플롯 예제의 두 번째 인사이트를 나타내는 그림에 대해서도 손쉽게\n적용이 가능합니다.\n\n\nage_40_text <- glue::glue(\n  \"Around the age of 40, we spent <br> less time with \n  <span style = 'color:{colors[2]};'>children<\/span> \n  and <br> more time <span style = 'color:{colors[1]};'>alone<\/span>.\"\n)\n\nage_40_plt +\n  labs(title = 'Label directly') +\n  annotate(\n    'richtext',\n    x = 15,\n    y = 350,\n    label = age_40_text,\n    hjust = 0,\n    vjust = 0,\n    family = 'firasans',\n    lineheight = 1.25,\n    size = 5,\n    label.color = NA\n  ) +\n  theme(legend.position = 'none')\n\n\n\n이런 완성도 있는 시각화는 청중 또는 대중들에게 우리가 전달하고자 하는\n이야기를 직관적으로 전달해줍니다. 범례와 플롯을 눈으로 왔다갔다 하는\n피로를 덜어주는 것은 덤이고요.\n마지막으로, 우리가 초기에 했던 예제인 막대그래프(bar chart) 예제에도\n이를 적용해봅시다. 해당 예제의 경우 다중 레이블에 해당하므로\nannotate()이 아닌 geom_text()가 필요로 됩니다.\n그런데, 어떤 이유에서인지.. 자동차 종류의 각 레이블 위치가 정렬이 안되는\n문제가 있었습니다. 불가피하게 연도별, 자동차 종류별 막대그래프의 높이를\n누적빈도(csum)로 계산하고, 레이블이 들어갈\n위치(n)를 적당하게 잡아주는 작업을 수행했습니다.🤯 코드가\n조금 복잡해 보이긴 하나, 출력된 결과를 확인하시면 어떤 작업을 진행했는지\n쉽게 이해하실 수 있을겁니다.\n\n\nmanual_counts <- mpg |> \n  count(year, class) |> \n  mutate(\n    year = factor(year),\n    class_group = case_when(\n      class %in% c('2seater', 'compact', 'midsize') ~ \"grp1\",\n      class == 'minivan' ~ \"grp2\",\n      T ~ \"grp3\"\n    )\n  ) \nmanual_counts\n\n# A tibble: 14 × 4\n   year  class          n class_group\n   <fct> <chr>      <int> <chr>      \n 1 1999  2seater        2 grp1       \n 2 1999  compact       25 grp1       \n 3 1999  midsize       20 grp1       \n 4 1999  minivan        6 grp2       \n 5 1999  pickup        16 grp3       \n 6 1999  subcompact    19 grp3       \n 7 1999  suv           29 grp3       \n 8 2008  2seater        3 grp1       \n 9 2008  compact       22 grp1       \n10 2008  midsize       21 grp1       \n11 2008  minivan        5 grp2       \n12 2008  pickup        17 grp3       \n13 2008  subcompact    16 grp3       \n14 2008  suv           33 grp3       \n\nlabels <- manual_counts |> \n  mutate(class = factor(class)) |>  \n  group_by(year) |> \n  arrange(year, desc(class)) |> \n  mutate(\n    csum = cumsum(n), \n    n = (lag(csum, default = 0) + csum) / 2\n  )\nlabels\n\n# A tibble: 14 × 5\n# Groups:   year [2]\n   year  class          n class_group  csum\n   <fct> <fct>      <dbl> <chr>       <int>\n 1 1999  suv         14.5 grp3           29\n 2 1999  subcompact  38.5 grp3           48\n 3 1999  pickup      56   grp3           64\n 4 1999  minivan     67   grp2           70\n 5 1999  midsize     80   grp1           90\n 6 1999  compact    102.  grp1          115\n 7 1999  2seater    116   grp1          117\n 8 2008  suv         16.5 grp3           33\n 9 2008  subcompact  41   grp3           49\n10 2008  pickup      57.5 grp3           66\n11 2008  minivan     68.5 grp2           71\n12 2008  midsize     81.5 grp1           92\n13 2008  compact    103   grp1          114\n14 2008  2seater    116.  grp1          117\n\n레이블이 들어갈 자리를 계산하는 키 아이디어는 lag()를\n통해서 계산한 누적빈도를 0값을 시작으로해서 한칸씩 당겨주고, 계산해둔\n누적 빈도(csum)를 더하여 2로 나눠주는 것입니다. 이 작업을\n수행하면 막대그래프를 구성하는 각 칸의 중간 높이를 계산할 수 있는\n것이죠.\n아울러, 우리가 본 시각화에서 한 가지 더 극복해야할 난관은 바로\n자동차의 종류 중 2seater의 빈도가 매우 작아서 레이블이\n들어갈 자리가 없는 점이 었습니다. 그래서, 2seater의 경우\n레이블을 막대의 맨 위에 표시되도록 하였습니다. 이러한 모든 난관들을\n극복하고 완성한 그림을 공개합니다.\n\n\nmanual_counts |> \n  ggplot(aes(x = year, y = n, fill = class_group)) +\n  geom_col(aes(alpha = class), col = 'white') +\n  scale_fill_manual(values = colors) +\n  scale_alpha_manual(values = alpha_vals) +\n  labs(\n    x = 'Year',\n    y = 'Counts',\n    alpha = 'Class',\n    title = 'Label directly'\n  ) +\n  # Add all but one label\n  geom_text(\n    data = labels |> filter(class != '2seater'),\n    aes(label = class), \n    col = 'white',\n    family = 'firasans',\n    size = 5,\n    fontface = 'bold'\n  ) +\n  # Add 2seater label\n  geom_text(\n    data = labels |> filter(class == '2seater'),\n    aes(y = n + 3, label = class), \n    col = 'black',\n    family = 'firasans',\n    size = 5,\n    fontface = 'bold'\n  ) +\n  theme(legend.position = 'none') \n\n\n\n맺음 말\n분석을 시작하며 혼자 가볍게 EDA를 하는 단계에서 본 예제와 같이\n시각화를 개선해나가는 작업은 필요로 되지 않을겁니다. 오히려 시간\n낭비일수도 있구요. 그러나, 내가 얻은 인사이트를 전달하는 자리 또는\n데이터를 기반으로 누군가를 설득해야하는 자리에서는 이 글에서 제공하는 몇\n가지 방법이 꽤나 도움이 될 것이라고 생각합니다. 물론, 전달하고자 하는\n내용이 한 눈에 들어도록 시각화를 수행하는 작업은 의외로 쉬울 때도\n있지만, 꽤나 까다로운 과정을 거쳐야하는 상황도 존재합니다. 실무에서는\n이와는 또다른 예상치 못한 까다로운 문제들을 겪는 상황들이 있을 수도\n있구요. 다만, 본 글에서 그림의 퀄리티를 단계단계 개선해나간 바와 같이\n전달하고자 하는 내용을 명확히하고 충분한 시간을 숙고해 그림을 개선해\n나간다면, 뭐든 해결할 수 있을 것이라고 봅니다. 하고자 하는 시각화를\n구현하지 못해낸다고 하더라도 그 과정 속에서 배우는 것은 분명히 존재할\n것입니다. 처음부터 완벽하게 아름다운 시각화를 해낼 수 있는 사람은 없다는\n것을 기억하셨으면 합니다.😁\n이번 포스팅을 준비하며 참고했던 글은 올해 봤던 데이터 시각화 관련\n아티클 중 제게 가장 큰 임팩트를 주는 글이었습니다. 누구나 하는 평범한\n시각화를 비범하게 만들어주는 글이라고 표현하면 적절할까요? 많은 사람들이\n알았으면 하는 내용이라, 8월 서울 R 미트업에서 본 내용을 주제로 발표를\n하기도 했습니다. 지금 이 글을 읽고 계신 여러분들에게도 좋은 인사이트를\n줄 수 있는 글이 되었으면 합니다.\n\n\n\nRapp, Albert. 2022. “4 Ways to Use Colors in Ggplot More\nEfficiently.” https://albert-rapp.de/posts/ggplot2-tips/07_four_ways_colors_more_efficiently/07_four_ways_colors_more_efficiently.html.\n\n\n🔗그래프\n문법이란?↩︎\n",
    "preview": "posts/2022-08-07-coloring-guide-for-ggplot2/preview.jpg",
    "last_modified": "2022-09-19T22:37:06+09:00",
    "input_file": "coloring-guide-for-ggplot2.knit.md"
  },
  {
    "path": "posts/2022-06-08-monthly-memory-202204/",
    "title": "월간 회고록: 2022년 4월",
    "description": "새로운 스터디, 이력서와 포트폴리오 제작기",
    "author": [
      {
        "name": "Taemo Bang",
        "url": {}
      }
    ],
    "date": "2022-06-08",
    "categories": [
      "Memory"
    ],
    "contents": "\n\nContents\n새로운 스터디를 시작하다\n이력서, 포트폴리오 제작기\n\n\n새로운 스터디를 시작하다\n올해 3월부터 SQL 스터디, Python 코딩테스트, Tensorflow 스터디를\n시작했습니다. 올 초부터 다양한 기업의 Data Scientist 채용 공고를\n둘러봤고, 아무래도 이 세 가지는 꼭 필요로 된다고 느꼈습니다. “왜 이제\n와서 시작하냐?” 하는 생각을 가지시는 분들이 많으실 것 같습니다. 작년에\n대학원을 졸업했고 실무에서 1년차를 넘긴 지금에서야 말이죠. 지금부터 그\n이야기를 풀어보려고 합니다. 사실, 지금 생각해보면 대학원 때 시작했어야할\n것을 이제서야 시작한다는게.. 참 많이 늦은 감있습니다. 하지만, 늦었을\n때가 가장 빠른?..뭐 이런 말로 위로를 삼아봅니다..\n사실 코딩테스트는 학부생 시절 대학원에 진학하기 전에 잠깐 취업 준비를\n해보면서, 대학원을 졸업하고 취업 준비를 하면서 몇 번 치뤘던 적이\n있습니다. R이 주 언어인 사람에게 다행스러웠던 것은 이때 치뤘던\n코딩테스트들에서는 다행히 R을 지원해줬었다는 점이죠. 두시간 세시간\n붙잡고 알고리즘 한두문제를 겨우 풀어서 제출했던 기억이 있습니다. 함수를\n다 짜서 제출하면 뭐하나요, 뭣도 모르고 입력을 받아야하는\ninput()도 안해서 테스트케이스는 다 틀리는데 말이죠.😅 네,\n당연히 항상 결과는 불합격이었습니다.\n그래서, 이제서야 시작한\n이유는?..\n참 부끄럽지만 “내가 이걸 왜 준비해야하지?”라는 고집같은 생각을\n했습니다. 내가 개발자도 아니고, Data science를 하고 싶은 사람인데 굳이\n알고리즘 문제를 왜 잘 풀어내야하지? 왜 이런 것을 요구하는 걸까? 하는\n생각을 했었죠. 지금 생각하면 참 바보같습니다. 아시다시피 요즘 나오는\n여러분들이 이름만 대면 알만한 대기업, 빅 플랫폼 기업, 금융 기관의 Data\nScientist 나 Data Analyst 채용 공고를 보시면 면접 전형 전에 꼭\n코딩테스트가 포함되어 있습니다.1 극 소수의 대기업에서는\n면접 전형 전 코딩테스트 대신 사전 과제 또는 Data Analyst의 경우 SQL 쿼리\n테스트를 진행하는 경우도 있긴 합니다만, 코딩테스트가 포함된 형태의 채용\n전형은 앞으로 기업들 사이에서 더더욱 확대될 것이라고 봅니다.\n과거에는 코딩테스트 공부는 거들떠보지 않았던 제가 지금에서야 공부를\n시작한 이유는 “내가 이걸 왜 준비해야하지?”와 같이 어리석은 고집같은\n생각을 버리고 그간 여러 생각을 해왔기 때문입니다. 먼저 “과연 내가 Data\nScience를 수행하기 위해 가고싶은 마음 속 업계 또는 기업만을 위해서 한\n노력이 있는가?”에 대해 생각했고, 수많은 지원자를 평가해야만하는 기업과\n실무자의 입장을 생각하기 시작하면서 제 관점은 많이 바뀌기 시작했습니다.\n대기업, 우리가 이름만 대면 알만한 핫한 기업에는 수많은 지원자가\n몰립니다. 그들의 입장에서 생각해보면, 다른 전형 없이 서류전형에서 각\n지원자들의 서류를 세세하게 평가하여 바로 면접 전형을 진행하는 것은 결코\n불가능합니다. 그래서, 코딩테스트와 같이 객관적인 평가 기준으로\n지원자들을 한 번 걸러내는 작업이 필요로 되는 것이라 생각합니다. 공기업\n채용 전형에서의 NCS, 사기업 채용 전형에서의 적성 평가2와\n같은 것과 같은 맥락으로, 개발 직군에게는 코딩테스트라는 것이 존재하는\n것이죠. 과거에는 이러한 형태의 채용 전형을 이해하고 싶지 않았습니다.\n기업의 Culture fit과 얼마나 맞는지에 관한 인성 검사와 같은 것들은 꼭\n필요로 된다고 생각했지만, NCS, 직무적성검사, 코딩테스트 같은 것들은\n실질적인 직무 수행 능력과 직결이 되는 것도 아닌데, 왜 치뤄야 하는지에\n대해 이해가 안됐었죠. 지금은 백 번 이해합니다. 오하려 과거에 되도 않는\n고집을 피우며 코딩테스트 공부를 거들떠보지 않았던 저를 참 한심하게\n생각하고있습니다.🤬\n아무튼 이러한 모티베이션에서 코딩테스트를 시작했고, SQL을 현업에서\n다루고 있지만 SQL 쿼리테스트 스터디도 시작을 했습니다. 아울러,\nTensorflow의 경우는 수많은 Data Scientist 채용공고를 둘러본 결과,\ntorch나 tensorflow 등과 같은 머신러닝 프레임워크 하나 정도는 다룰 줄\n알아야 될 것 같음을 느껴 시작하게 됐습니다. 여러 프레임워크 중\nTensorflow를 선택한 이유는, 현재 M1 GPU를 지원해주는 유일한\n프레임워크이기 때문입니다. 그마저도 싱글코어긴 합니다..(사실 torch를\n배워보고 싶었는데,,) 그리고, R의 {tidymodels}을 통해\n머신러닝을 수행할 수 있긴 합니다만, 우리나라 업계의 Data Scientist 채용\n공고에서 아직 R의 {tidymodels}를 기재해놓은 공고는 본 적이\n없습니다. Python의 scikit-learn을 요구하는 경우는 종종\n봤지만 말이죠. 참 씁쓸하네요..😭 개인적으로 {tidymodels}은\nscikit-learn과 비교하기 미안할 정도로 더 좋은 패키지인데\n말이죠. 아무튼, Tensorflow 스터디는 4월에 아카이브를 만들어 놓고, 업무와\n다른 일을 핑계로 아직도 제대로 시작하지 않고 있네요.. 마침 Deep\nLearning with R, Second Edition이 곧 출판을 앞두고 있다는 소식을\n들었는데, 이 책으로 스터디를 진행할까 합니다. 아니, 해야죠!\n🔗SQL\n스터디\n🔗Python\n코딩테스트 스터디\n🔗Tensorflow\n스터디\n이력서, 포트폴리오 제작기\n기존에는 canva로 이력서와\n경력기술서를 관리하고, 포트폴리오는 애플 키노트로 관리하고 있었는데\n하나의 툴로 관리하고 싶었어요. R 마크다운과 노션 중에 고민하다가\n노션으로 택했습니다. R Markdown에 비해 웹 공유도 편하고, PDF 변환,\n그리고 무엇보다 디자인적인 요소가 훨씬 낫다고 생각했습니다. 그리고, R\nMarkdown으로 관리했을 때 얻을 수 있는 베네핏도 딱히 없다고 생각했고,\n이력서뿐만이 아니라 포트폴리오까지 함께 관리하기엔 노션이 확실히\n편합니다. 이번에 노션으로 이력서와 포트폴리오를 다시 쭉 작성하며\n참고해봤던 자료들입니다:\n🔗개발자 이력서\n작성하기\n🔗eo -\n최고의 직장에서 깨달은 내 몸값을 높이는 스킬 | 커리어 액셀러레이터\n김나이\n🔗Data\nScientist 김단아님 노션 Resume\n참고할만한 노션 Resume의 99%는 개발자 이력서이고 나머지는 통계학과\n외에 다른 백그라운드로 Data Science를 하시는 분들의 이력서 뿐인데,\n김단아님은 저와 같은 통계학 백그라운드로 Data Science를 하시는 분이라 참\n많은 도움이 됐습니다. 이력서, 포트폴리오를 만들고 다듬는데에 대략\n4일정도 걸린 것 같습니다. 이미 작성된 이력서, 경력기술서, 포트폴리오가\n있었음에도 불구하고, 지겹고 힘들더군요.😪 참고했던 글, 영상 들에서\n공통적으로 주장하는 이력서와 포트폴리오의 주요 포인트는 다음과\n같습니다:\n내가 “어떤 것을 했다.”와 같이 팩트만 펼처 놓는 것이 아닌, 나의\n강점을 펼치고 상대방을 설득할 수 있도록 기술하자\n이력서는 영화 예고편과 같다. 짧고 간결하게 꼭 보여주고 싶은\n것들만 컴팩트하게 담자\n경험과 직무를 연결하자\n가능하다면 숫자로 성과를 드러내라\n숫자로 표현할 수 없다면, 그 일을 왜 했는지, 타겟이 누구였는지\n디테일하게 담아보자\n\n개발 직군의 경우 다룰줄 아는 Tool의 수준을 나타내는 것은\n지양하자\nTool의 수준에는 주관이 개입하기 마련이고, 객관적인 기준이 없기\n때문\n개인적으로 주 언어정도를 표기하는 것은 나쁘지않다고 봄\n나머지 본인이 다루는 각 Tool의 수준은 포트폴리오에서 자연스럽게\n드러나야함\n\n경력 기술, 포트폴리오 작성 시 Data Privacy, Research Privacy,\n업무 상 비밀은 꼭 지켜야 함\n이를 지키지 않으면 이력서를 평가하는 사람 입장에서도 큰 (-)가 될 수\n있음\nPrivacy를 지키기 위해 마스킹이 필요한 부분은 꼭 마스킹하여\n기술하자\n\n버려야 하는 내용은 과감하게 버려야하는데, 이게 참 어려웠던 것\n같습니다. Privacy를 지키는 일도 매우 중요한데, 꽤 귀찮았고요.😅\n이직을 계획하고 계신 분들이 아니여도 이력서, 포트폴리오를 틈틈히\n정리해두는 습관은 꼭 필요합니다. 이력서와 포트폴리오가 꼭 필요한 상황에\n닥쳐서 한꺼번에 지금까지 해온 것들을 정리하는 작업은 정말 힘든 일입니다.\n정말 많은 시간이 소요될 것이고, 사람의 기억력에는 한계가 있기 때문에\n틈틈히 주기적으로 이력서와 포트폴리오를 관리해온 사람에 비해 좋은\n퀄리티를 갖기도 힘들 것입니다. 더군다나, 요새는 “평생직장”이 아닌\n“평생직업”을 바라보고 살아가야하는 세상이기에 본인 PR을 할 줄\n알아야합니다. 과장 좀 보태서 이야기 해보면, 본인이 한 것은 100인데\n50으로 밖에 포장을 못하는 사람이 있는 반면, 본인이 한 것은 70인데\n100만큼 포장할 줄 아는 사람이 있습니다. 본인이 어디쯤 위치하는 사람인지\n곰곰이 생각해보시기 바랍니다. 그래서, 커리어를 쌓아가는 데에 있어서\n본인이 이루어 낸 것들을 주기적으로 잘 정리하고 포장하는 것은 기본 중의\n기본이라 생각합니다. 이직 계획과는 무관하게 적어도 분기에 1번 정도는\n이력서와 포트폴리오의 유지보수에 시간을 투자하는 것을 적극 권장합니다.\n나라는 상품을 취업 시장에 내놓는데, 다른 상품들과의 차별점을 꾀하기 위해\n이정도 노력은 꼭 필요하지 않겠습니까? 이런 노력 없이도 남들보다 훨씬 더\n뛰어난 무언가를 갖고 있는 인재가 아닌 이상 말이죠.\n마지막으로 4일 간의 끈질긴 작업 끝에 완성한 제 이력서와 포트폴리오\n링크를 첨부하면서 회고를 마칩니다. 앞서 말씀드렸던 사항들을 최대한\n지키려고 노력했지만, 잘 지켜졌는지.. 틈틈히 들여다 보고 유지보수\n해나가려고 합니다.\n🔗방태모의\n이력서\n\n반면에, 규모가 조금 작고 지원자가\n대기업에 비해 적은 수준에 머무르는 스타트업의 경우는 대개 면접 전형\n이전에 서류 전형과 사전 과제 전형을 통해 지원자를 평가합니다.↩︎\n삼성의 GSAT, 현대의 HMAT과 같은↩︎\n",
    "preview": "posts/2022-06-08-monthly-memory-202204/preview.jpg",
    "last_modified": "2022-09-19T22:36:23+09:00",
    "input_file": "memory-202204.knit.md"
  },
  {
    "path": "posts/2022-05-24-paper-review-simes-et-al-2022/",
    "title": "논문 요약 - Simões et al (2022)",
    "description": "프랑스 남부 지역의 심호흡곤란 입원 발생에 관한 대기오염원 영향 평가 논문 요약",
    "author": [
      {
        "name": "Taemo Bang",
        "url": {}
      }
    ],
    "date": "2022-05-24",
    "categories": [
      "Time Series",
      "Paper"
    ],
    "contents": "\n\nContents\n1st read\n맺음말\n\nPrerequisite: 논문\n요약 방식\n\n1st read\nCategory\nResearch paper\nMain Topic\n제목\nCardiac dyspnea risk zones in the South of France identified by\ngeo-pollution trends study - (Simões et al. 2022)\n주제\n프랑스 남부 지역의 Cardiac dyspnea(CD, 이하 심호흡곤란) 발생에\n미치는 대기오염원(\\(\\rm{PM}_{10}\\),\n\\(\\rm{NO}_{2}\\), \\(\\rm{O}_{3}\\)) 영향 평가\nContext\n선행 연구들에서 대기오염원에 관한 단기 노출이 심근경색(myocardial\ninfarction), 울혈성심부전(congestive heart failure)과 같은 몇몇 심혈관\n병리(cardiovascular pathologies)들에 미치는 영향을 평가하긴 했으나,\n심호흡곤란의 경우 이러한 관계를 아직 완전히 입증하지 못함\n따라서, 본 연구의 목적은 대기오염원, 기상요인, 심호흡곤란 입원\n데이터를 활용해 심호흡곤란 입원 발생 원인에 관한 메커니즘을 알아보고,\n이를 예방하기 위한 정책을 개발하는 것에 있음\n본 연구의 주요 방법론은 Distributed\nlag non linear model(이하, DLNM)과 메타분석(Meta analysis)에\n해당함\nCorrectness\n기상요인(meteorological factors)들을 공변량(coviariates)으로\n활용하는데, 다중공선성(multicollinearity)을 피하기 위해 상관이\n존재할만한 두 변수 중 하나의 변수만 모형에 포함시킴\n최대 지연 효과(maximum lag days)는 14일까지 고려하였으나, 이에\n관한 합리적 근거는 없다고 보여짐\nContributions\n프랑스 남부 전체 지역의 심호흡곤란 입원 발생에 관한 대기오염원의\n영향을 평가한 첫 번째 연구\n\\(\\rm{NO}_2\\), \\(\\rm{O}_3\\), \\(\\rm{PM}_{10}\\)에 단기 노출이 심호흡곤란으로\n인한 응급실 방문을 증가시킨다는 것에 관한 유의한 증거 제시\n본 논문의 접근 방식은 공중 보건 정책에 관한 예측 도구로서\n대기오염원 모니터링을 효과적으로 제안함\nClarity\n지금까지 읽어본 바로는 명료하게 잘 쓰인 논문이라 생각됨\n맺음말\n본 논문을 통해 실제 각 도시별 DLNM을 이용한 대기오염원 건강영향평가\n수행 후, 메타분석으로 오버롤한 결과를 제시할 수 있음을 확인했습니다.\n메타분석을 어떻게 진행하였는지에 관한 이론적 부분은 자세하게 기술되어\n있지 않아서 두 번째 읽기는 진행하지 않았으나, 도시별 분석 결과를\n메타분석을 통해 종합할 수 있다는 것을 확인하는 것으로는 첫 번째 읽기로도\n충분했습니다.\n본 논문에 쓰인 메타분석은 일반적으로 임상연구에서 수행하는 메타분석을\n다양한 상황에 쓸 수 있도록 일반화하여 확장시킨 형태의 메타분석\n방법론이라고 보시면 됩니다. 해당 방법론을 깊이있게 이해하기 위해서는\n(Sera et al. 2019)을 참고하시면 됩니다.\n해당 논문의 예제 R 소스코드는 여기를\n참고하시면 됩니다. 다양한 형태의 분석을 수행한 뒤에\nlibrary(mixmeta)를 통해 메타분석을 수행하여 결과를 종합하는\n과정을 보여준다는 점에서 큰 의미가 있습니다. 그러나, 정작 제가\n필요로하는 DLNM으로 건강영향평가를 도시별로 수행한 뒤에 메타분석을 하는\n소스코드는 없다는 점이 조금 아쉬웠습니다.😂 그래서, 추가적으로 (Gasparrini, Armstrong,\nand Kenward 2012)에서 제공하는 R\n예제 소스코드를 함께 참고했습니다. 확장된 형태의 메타분석인 (Sera et al. 2019)가 나오기 전이라\nlibrary(mvmeta)를 통해 분석이 진행되긴 합니다만,\nlibrary(mixmeta)와 똑같은 로직으로 분석이 진행되기 때문에\n해당 소스코드를 함께 참고하시면 도시별 DLNM 분석 결과를 메타분석하는\n것을 어렵지 않게 구현하실 수 있을겁니다.\n\n\n\nGasparrini, A., B. Armstrong, and M. G. Kenward. 2012.\n“Multivariate Meta-Analysis for Non-Linear and Other\nMulti-Parameter Associations.” Statistics in Medicine 31\n(29): 3821–39. https://doi.org/10.1002/sim.5471.\n\n\nSera, Francesco, Benedict Armstrong, Marta Blangiardo, and Antonio\nGasparrini. 2019. “An extended mixed-effects framework for\nmeta-analysis.” Statistics in Medicine 38 (29): 5429–44.\nhttps://doi.org/10.1002/sim.8362.\n\n\nSimões, Fanny, Charles Bouveyron, Damien Piga, Damien Borel, Stéphane\nDescombes, Véronique Paquis-Flucklinger, Jaques Levraut, Pierre Gibelin,\nand Silvia Bottini. 2022. “Cardiac Dyspnea Risk Zones in the South\nof France Identified by Geo-Pollution Trends Study.”\nScientific Reports 12 (February). https://doi.org/10.1038/s41598-022-05827-2.\n\n\n\n\n",
    "preview": "posts/2022-05-24-paper-review-simes-et-al-2022/preview.jpg",
    "last_modified": "2022-09-19T22:36:02+09:00",
    "input_file": "paper-review-simes-et-al-2022.knit.md"
  },
  {
    "path": "posts/2022-05-13-how-to-review-a-paper/",
    "title": "관심 논문 읽고 요약하기",
    "description": "논문을 읽고 요약하는 나만의 방식",
    "author": [
      {
        "name": "Taemo Bang",
        "url": {}
      }
    ],
    "date": "2022-05-13",
    "categories": [
      "Paper"
    ],
    "contents": "\n\nContents\n1st read\n2nd read\n맺음말\n\n\n이 글은 (An 2022)을 기반으로\n작성되었습니다.\n논문 읽기가 초심자에게는 만만치않은 작업인 만큼, 논문을 정리하는\n자기만의 방식을 만들어 놓는 것은 참 중요합니다. 저 또한 아직 초심자라고\n생각하고 있는데요, 오늘은 제가 관심있는 논문을 읽고 정리하는 방식에 대해\n얘기해보려고 합니다. 제가 스스로 터득한 방법을 소개드리는 것은 아닙니다.\n논문을 읽고 정리하는 좋은 방식이 있나 싶어 검색을 하던 도중 좋은 글(An 2022)을 발견하게 됐고, 이 글을\n바탕으로 저만의 방식을 정립해봤습니다. 좋은 글을 써주신 안수빈님께\n감사의 마음을 전합니다.\n논문 요약은 1st read(첫 번째 읽기), 2nd\nread(두 번째 읽기) 2가지 섹션으로 진행할 것입니다. 1st\nread의 결과에 따라 2nd read는 진행되지 않을\n수도 있습니다.\n1st read\n첫 번째 읽기의 핵심은 빠르게 읽으며 논문의 큰 그림을 파악하는\n것이라고 합니다. 5분에서 10분 정도 다음 순서에 따라 읽으라고\n권합니다.\n제목(title), 초록(abstract),\n소개(introduction)를 집중해서 읽으세요.\n섹션(section)과 하위\n섹션(subsection)의 세부내용은 무시하고 제목만\n읽으세요.\n결론(conclusion)을 읽으세요.\n참고문헌(reference)을 보며 저자가 인용한 논문,\n이전에 읽은 논문에 대해 가볍게 체크하세요.\n먼저 1st read에서는 논문을 빠르게 읽으면서 파악한\n전반적인 그림에 관해 기술합니다. 첫 번째 읽기를\n하고나서는 다음의 여섯 가지(5C + 1M)를\n답할 줄 알아야 하며, 본 블로그에서 요약할 논문들 또한 다음과 같은\n섹션으로 요약하려고 합니다.\nCategory: 논문의 종류\nMain Topic: 논문 제목 및 주제\nContext: 다른 페이퍼들과의 관계, 문제를 풀기\n위해 사용한 이론적 바탕\nCorrectness: 논문에 필요한 가정의\n명확성\nContributions: 논문의 핵심 기여\nClarity: 논문의 가독성, 명료함\n논문에 필요한 가정의 명확성(Correctness)은 제 경우\n보통 방법론 부분에서 모델에서 요구하는 가정이나 모델링 과정의 각 단계가\n합리적인 근거로 진행 되었는지에 관심이 있으므로, 첫 번째 읽기에서\nMethod 부분을 빠르게 검토해보는 과정이 필요로 될 것\n같습니다. 아울러, 논문의 가독성과 명확성(Clarity)에\n관한 부분은 잘 아는 분야가 아니라면 감히 기술하기 어려울 것 같습니다.\n때때로 생략할 수도 있는 부분입니다.😅 그리고, 논문의\n종류(Category)는 이 글(Hong\n2012)을 참고하세요. 5C +\n1M을 바탕으로 논문을 더 읽을지 말지 선택할 것입니다. 더\n읽지 않는 결정을 한다면, 대부분은 다음의 이유일 겁니다.\n관심이 없는 내용\n해당 논문을 읽기엔 사전 지식이 부족\n저자의 가정이 모호 또는 불명확\n만약, 꼭 읽어야만 하는 논문임에도 해당 논문을 읽기에 사전 지식이\n부족하다면, 참고문헌(reference)들을 다시 검토해보면서 관심있는 연구\n분야의 핵심 연구라고 생각 되는 것을 찾아내 읽어보는 과정을 가져야\n할겁니다. 또는, 논문에 쓰인 방법론에 관한 이해가 안되어 있는 상태라면\n해당 방법론의 Method paper나 Review paper를 찾아보는 것도 큰 도움이 될\n겁니다.\n2nd read\n2nd read에서는 좀 더 세부적인 내용에 집중하라고\n합니다. 단, 증명같은 디테일은 무시한채 말이죠. 핵심 사항을 노트에 적거나\n테두리에 본인의 의견을 써놓는 것을 권장합니다. 두 번째 읽기는 약 1시간\n정도가 소모됩니다. 처음 접하는 분야의 논문이나 개인의 논문 독해 실력에\n따라 훨씬 더 많은 시간이 소요될 수도 있습니다. 다음과 같은 사항에\n주목하여 읽으세요.\nFigure, Diagram, Table 등 논문\n내 다양한 도표와 일러스트레이션을 주의깊게 보세요. 특히, Data Science에\n관심이 있는 분들이라면 그래프를 잘 봐야합니다. 그래프의 \\(x\\)축, \\(y\\)축, 테이블의 행과 열이 의미하는 바 등을\n확인하고 이를 통해 저자가 주장하고자 하는 바가 무엇인지 한마디로 정리할\n줄 알아야합니다. 물론, 이 부분은 저자가 확실하게 주장하고자 하는 바를\n가지고 시각화, 테이블 작성를 수행했다는 전제 하에 있습니다.\n아직 읽지 않은 연관 논문을 체크하세요. 이 과정은 논문의 배경 지식\n또는 특정 방법론에 관한 Method paper인 경우 해당 방법론의 모티베이션을\n공부하는데 도움이 됩니다.\n두 번째 읽기가 끝난 상태에서 우리가 바라는 희망사항은 다음과\n같습니다:\n논문의 핵심 내용 이해\n논문의 핵심 주장에 대해 근거와 함께 요약할 수 있어야 함\n그래서, 두 번째 읽기를 끝낸 논문은 다음과 같은 섹션으로 상세한 추가\n요약을 수행할 예정입니다.\nMain Findings: 논문의 핵심 주장과 뒷받침\n근거\nMethods: Main Findings에 사용된\n핵심 방법론에 관한 내용\nResults: Main Findings외 다른\n연구 결과\nLimitations: 연구의 한계점\nMain Findings외 다른 연구 결과에 해당하는\nResults와 연구의 한계점(Limitations)는\n때때로 생략될 수 있습니다.\n두 번째 읽기는 당신이 관심있어 하지만, 당신의 전문 연구 분야는 아닌\n논문에 적합하다고 합니다. 하지만, 저는 제 전문 연구 분야도 위와 같은 두\n번째 읽기를 통해 추가적으로 세부적인 요약을 수행할 예정입니다. 전문 연구\n분야라면 훨씬 더 빠르게 두 번째 읽기를 할 수 있겠죠. 그러나, 여러 이유로\n두 번째 읽기에도 이해가 안될 수도 있습니다:\n이 주제나 내용이 새로워서 전문 용어나 약어에 익숙하지\n않음\n저자가 사용한 방법론이나 연구 결과를 낼 때 사용된 테크닉이 이해가\n안됨\n합리적 근거가 부족한 주장 또는 너무 많은 레퍼런스\n피곤해서!\n이럴 때 3가지 선택지를 제안합니다.\n논문을 치우세요. 그리고, 해당 논문의 내용이 커리어에 무관하기를\n바라세요.\n배경 지식을 공부하고 다시 읽으세요.\n노력해보고 세 번째 읽기를 해보세요.\n거인의 어깨 위에 올라서서 세상을 바라보라고 하는데, 거인에 어깨 위에\n올라서는 것 조차 참 어렵습니다..😭\n맺음말\n앞으로 제 블로그에 읽은 논문들을 요약하는 글을 작성하기에 앞서, 논문\n요약 방식에 대한 설명이 필요할 것 같아서 쓰게 된 글입니다. 논문 요약\n방식에 정답은 없습니다. 각자의 논문 요약 방식에 대해 나눠보는 것도 참\n흥미로운 대화 거리가 될 것 같네요. 참고한 글(An\n2022)에 더 좋은 내용이 많습니다. 그리고, 해당 글의 세\n번째 읽기, 문헌 조사 등 “논문\n쓰기”에 도움이 될 만한 내용들 또한 기술이 되어있습니다. 다시 한\n번 좋은 글 작성해주신 안수빈님께 감사의 말씀을 전합니다. 저도 아직 많이\n부족하지만, 이 글이 첫 논문을 접하는 분들께 조금이나마 도움이 됐으면\n합니다.🙏\n\n\n\nAn, Subin. 2022. “[Paper Review] How to Read a Paper (SIGCOMM\n07).” https://ansubin.com/paper-review-how-to-read-a-paper/.\n\n\nHong, Seonui. 2012. “02. 논문의 종류.” http://www.campuskorea.net/bbs/board.php?bo_table=any_tip1&wr_id=30.\n\n\n\n\n",
    "preview": "posts/2022-05-13-how-to-review-a-paper/preview.jpg",
    "last_modified": "2022-09-19T22:35:32+09:00",
    "input_file": "how-to-review-a-paper.knit.md"
  },
  {
    "path": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/",
    "title": "tidyverse로 데이터베이스랑 대화하기 - 1편",
    "description": "R을 통해 데이터베이스 관련 작업을 한다면, 꼭 SQL 전문가가 될 필요는 없어요!",
    "author": [
      {
        "name": "Taemo Bang",
        "url": {}
      }
    ],
    "date": "2022-04-07",
    "categories": [
      "SQL",
      "R"
    ],
    "contents": "\n\nContents\n1 DB 연결하기\n예제용 토이\nDB\nSQLite DB\n연결하기\n\n2 DB\n둘러보고 다뤄보기\nDBI 함수\ndplyr 함수\ndplyr 문법을 SQL\n쿼리문으로\n출력문의\nlazy query / ??의 의미\n\n3 DB 연결\n해제하기\n다음 파트에서 배울 내용\n\n\n\n본 글은 (Naidoo 2020)를\n기반으로 작성되었습니다.\n실무에서는 Data analyst, Data scientist를 가리지 않고 SQL에 관한\n능력을 요구합니다. 우리나라의 채용공고를 둘러보면 Data analyst의 경우\n특히 SQL 스킬을 중요하게 요구하는 듯 합니다. 방대한 양의 데이터를\n저장하고 관리하기 위해 실무에서는 데이터베이스를 사용합니다.\n데이터베이스는 종종 관계형 데이터베이스 시스템1(이하 RDBMS)이라 불리기도 하죠.\n그리고, 우리는 SQL2 언어 또는 SQL을 조금 변형한(variant)\n언어를 통해 이 데이터베이스에 질의(query)를 합니다. 여기서 변형이라는\n말을 사용한 이유는, RDBMS를 제공하는 업체에서 표준화된 SQL을 제공하는\n경우도 있지만, 표준화된 SQL을 조금 변형시켜 사용하는 경우도 있기\n때문입니다.\n만약 이렇게 특정 업체로부터 제공되는 변형된 RDBMS를 사용해야한다면,\n해당 업체에서 사용하는 특정 SQL dialect3를\n사용해 쿼리를 작성하는 방법을 이해해야 하실겁니다. 변형된 RDBMS를 예로\n들어보자면, PostgreSQL, PrestoDB(AWS의\nAthena를 위한) 등이 있습니다. PostgreSQL DB의 JSON 필드는 AWS에서\n구조화된 중첩 배열로(array) 수집되므로, 동일한 필드를 쿼리하고자 할 때\n다른 쿼리문을 사용합니다.\nR을 사용하는 여러분 모두 잘 아시다시피, R에서는\n{dplyr}4 패키지를 통해 이러한 작업을 데이터에\n수행할 수 있습니다. {dplyr}이 select(),\ngroup_by(), left_join() 등 SQL 문법을 잘\n모방하긴 했지만, SQL 문법과 R 문법 사이를 완벽하게 왔다갔다 하기는\n어렵습니다. 예를 들자면, {dplyr}의 filter()를\n이용해 특정 행을 뽑아올 때, 우리는 R 문법을 따라야하므로 조건문에\n=이 아닌 ==을 사용하죠. 이는 SQL 문법과는\n완벽히 다른 부분입니다.\n자, 여기서 이러한 상황을 타개할 방법은 무엇일까요. 엄청난 용량의\n데이터베이스를 R로 가져올 수는 없습니다. 메모리 베이스인 R에 이 짓을\n햇다가는요? 생각도 하기 싫습니다.😰 그럼, RDBMS 환경에서 이러한 무거운\n작업을(e.g. computation) 수행하고 필요로 될 때에만 R에다가 가져오면 되지\n않을까요? 예를 들면, 집계된 데이터를 가져와서 보고서용 그림을\n그린다든지. 이를 가능하게끔 해주는 패키지에 대해 배워보려고 합니다.\n본 튜토리얼에서는 {dplyr}의 데이터베이스 백엔드 버전이라\n할 수 있는 {dbplyr} 패키지에 대해 배울거에요.\n{dbplyr}은 당신의 RDBMS에 R의 tidyverse 문법을 사용한\n쿼리문을 직접적으로 사용할 수 있게끔 해줄겁니다.😀\n1 DB 연결하기\n먼저 필요한 패키지를 불러오죠.\ninstall.packages(\"패키지명\")을 통해 설치할 수 있습니다.\n\n\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(dplyr)\nlibrary(RSQLite)\nlibrary(odbc)\n\n\n{DBI}: R의 데이터베이스 인터페이스에 관한 메인\n패키지입니다.\n{dbplyr}: {dplyr} 문법을 사용하여\n데이터베이스에 질의를 할 수 있게끔 해줍니다.\n{dplyr}: 데이터베이스에 질의할 때 사용할\n패키지입니다.\n{RSQLite}: 가벼운 단일 유저용 데이터베이스 SQLite DB에\n연결할 수 있게끔 해주는 DBI5 호환 패키지입니다.\nR-SQLite로 이해하시면 편합니다.\n다른 DBI 용 호환 패키지가 필요할 수도 있습니다. 예를 들어,\n{RPostgres}는 PostgreSQL RDBMS와 연결을 해주는\n패키지입니다.6\n{odbc}: odbc 드라이버를 사용해 RDBMS 인터페이스에\n인터페이스할 수 있도록 해주는 DBI 호환 인터페이스입니다.7\n예제용 토이 DB\nAlison Hill이 The Great British Bake\noff에서 만든 데이터를 사용하려고 합니다. 본 예제에서 다룰\n데이터베이스는 여기서\n내려받으세요. {bakeoff} 패키지의 데이터를 이용해 연습에\n사용할 SQLite DB를 만들었습니다. 이 튜토리얼의 원 저자 Vebash\nNaidoo님께 감사의 말을 전합니다.\nSQLite DB 연결하기\n이제 DB를 SQLite DB에 연결해봅시다. DB와 대화를 나누기 위해서, 우선\n연결(connection)을 해줘야합니다. 다음의 작업을 해줄겁니다.\nDBI 패키지 로딩: library(DBI)\n연결하기:\ncon <- dbConnect(RSQLite::SQLite(), \"내려받은 db 경로\")\n\n\nlibrary(DBI) # main DB interface\nlibrary(dplyr) \nlibrary(dbplyr) # dplyr back-end for DBs\n\ncon <- dbConnect(drv = RSQLite::SQLite(), # give me a SQLite connection\n        dbname = \"data/great_brit_bakeoff.db\")\nsummary(con) # What do we have?\n\n          Length            Class             Mode \n               1 SQLiteConnection               S4 \n\n위와 같은 명령어가 출력되면 DB에 성공적으로 연결된 것입니다.\n2 DB 둘러보고 다뤄보기\n자, DB 연결도 했으니 이제 몇 가지 DBI 함수를 이용해 연결한 DB를\n둘러보고 다뤄봅시다.\nDBI 함수\nDBI 함수들의 이름은 꽤 직관적입니다.\n\n\ndbListTables(con) # 연결된 테이블 리스트를 보여줘!\n\n [1] \"baker_results\"     \"bakers\"            \"bakes\"            \n [4] \"challenge_results\" \"challenges\"        \"episode_results\"  \n [7] \"episodes\"          \"ratings\"           \"ratings_seasons\"  \n[10] \"results\"           \"seasons\"           \"series\"           \n\n\n\ndbListFields(con, # 연결한 DB로 가서\n      \"bakers\")   # bakes 테이블에 어떤 필드가 있는지 알려줘!\n\n[1] \"series\"     \"baker_full\" \"age\"        \"occupation\" \"hometown\"  \n\nDB에 질의는 다음과 같이 수행할 수 있어요.\n\n\nres <- dbSendQuery(con, \"SELECT * FROM bakers LIMIT 3\") # 쿼리문 실행\n# bakers 테이블에 모든 필드를 가져오는데, 관측치 3개까지만 가져와봐!\ndbFetch(res) # 결과 출력해줘\n\n  series          baker_full age                        occupation\n1      1       Annetha Mills  30                           Midwife\n2      1      David Chambers  31                      Entrepreneur\n3      1 Edward \"Edd\" Kimber  24 Debt collector for Yorkshire Bank\n       hometown\n1         Essex\n2 Milton Keynes\n3      Bradford\n\n\n\ndbClearResult(res) # 결과 지우기\n\n\ndplyr 함수\n이제, 우리가 잘하는 {dplyr}의 함수들을 이용해 마음껏\nDB와 이야기해보죠.\ndplyr::tbl(con, \"테이블명\"): 연결한\nDB(con)으로 가서 SELECT * FROM 테이블명\n실행해줘.\n\n\ntbl(con, \"bakers\")\n\n# Source:   table<bakers> [?? x 5]\n# Database: sqlite 3.39.1 [/Volumes/Essential/Study/Private/Writing/Distill/_posts/2022-04-07-talk-with-database-using-tidyverse-part-i/data/great_brit_bakeoff.db]\n   series baker_full                age occupation             homet…¹\n    <dbl> <chr>                   <dbl> <chr>                  <chr>  \n 1      1 \"Annetha Mills\"            30 Midwife                Essex  \n 2      1 \"David Chambers\"           31 Entrepreneur           Milton…\n 3      1 \"Edward \\\"Edd\\\" Kimber\"    24 Debt collector for Yo… Bradfo…\n 4      1 \"Jasminder Randhawa\"       45 Assistant Credit Cont… Birmin…\n 5      1 \"Jonathan Shepherd\"        25 Research Analyst       St Alb…\n 6      1 \"Lea Harris\"               51 Retired                Midlot…\n 7      1 \"Louise Brimelow\"          44 Police Officer         Manche…\n 8      1 \"Mark Whithers\"            48 Bus Driver             South …\n 9      1 \"Miranda Gore Browne\"      37 Food buyer for Marks … Midhur…\n10      1 \"Ruth Clemens\"             31 Retail manager/Housew… Poynto…\n# … with more rows, and abbreviated variable name ¹​hometown\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\ntbl(con, \"bakers\") %>% \n    head(3) # \"SELECT * FROM bakers LIMIT 3\"와 동일\n\n# Source:   SQL [3 x 5]\n# Database: sqlite 3.39.1 [/Volumes/Essential/Study/Private/Writing/Distill/_posts/2022-04-07-talk-with-database-using-tidyverse-part-i/data/great_brit_bakeoff.db]\n  series baker_full                age occupation              homet…¹\n   <dbl> <chr>                   <dbl> <chr>                   <chr>  \n1      1 \"Annetha Mills\"            30 Midwife                 Essex  \n2      1 \"David Chambers\"           31 Entrepreneur            Milton…\n3      1 \"Edward \\\"Edd\\\" Kimber\"    24 Debt collector for Yor… Bradfo…\n# … with abbreviated variable name ¹​hometown\n\n데이터베이스와 대화를 나눌 때 마다 초기에 연결해둔 con을\n사용한다는 점을 유념해주세요. 초기에 불러왔던 con은\n아까처럼 일반적인 SQL 쿼리문을 이용해 질의를 할 때 뿐만이 아닌\n{dplyr}을 통해 타이디한 파이프라인으로 원하는 테이블을\n가져올 때도 사용됩니다.\n자 이제 예시 상황을 하나 들어서 {dplyr}로 원하는\n테이블을 가져와보겠습니다. baker_results 테이블에는 각 제빵\n대회에 참가한 제빵사(baker)의 세부 정보 필드가 담겨있습니다:\n\n\ndbListFields(con, \"baker_results\")\n\n [1] \"series\"                    \"baker_full\"               \n [3] \"baker\"                     \"age\"                      \n [5] \"occupation\"                \"hometown\"                 \n [7] \"baker_last\"                \"baker_first\"              \n [9] \"star_baker\"                \"technical_winner\"         \n[11] \"technical_top3\"            \"technical_bottom\"         \n[13] \"technical_highest\"         \"technical_lowest\"         \n[15] \"technical_median\"          \"series_winner\"            \n[17] \"series_runner_up\"          \"total_episodes_appeared\"  \n[19] \"first_date_appeared\"       \"last_date_appeared\"       \n[21] \"first_date_us\"             \"last_date_us\"             \n[23] \"percent_episodes_appeared\" \"percent_technical_top3\"   \n\n각 제빵대회 우승자의 출신이 영국의 일부 지역에서 나왔는지, 아니면\n다양한 지역으로부터 우상자가 배출되었는지 알고싶은 상황이라고 해봅시다.\n우선 다음과 같이 관심있는 필드만 불러와주겠습니다.\n\n\ntbl(con, \"baker_results\") %>% \n  select(series, baker, hometown, series_winner)\n\n# Source:   SQL [?? x 4]\n# Database: sqlite 3.39.1 [/Volumes/Essential/Study/Private/Writing/Distill/_posts/2022-04-07-talk-with-database-using-tidyverse-part-i/data/great_brit_bakeoff.db]\n   series baker     hometown              series_winner\n    <dbl> <chr>     <chr>                         <int>\n 1      1 Annetha   Essex                             0\n 2      1 David     Milton Keynes                     0\n 3      1 Edd       Bradford                          1\n 4      1 Jasminder Birmingham                        0\n 5      1 Jonathan  St Albans                         0\n 6      1 Lea       Midlothian, Scotland              0\n 7      1 Louise    Manchester                        0\n 8      1 Mark      South Wales                       0\n 9      1 Miranda   Midhurst, West Sussex             0\n10      1 Ruth      Poynton, Cheshire                 0\n# … with more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n24개 열 중 관심있는 4개 열만 불러왔습니다. 이제 제빵대회에 우승한\n사람만 골라낸 뒤(filter()) 우승자들이 영국의 어떤 지역으로\n부터 왔는지 지역별로 인원을 구하고(count()) 내림차순\n정렬(sort())을 해보죠.\n\n\ntbl(con, \"baker_results\") %>% \n  select(series, baker, hometown, series_winner) %>% \n  filter(series_winner == 1) %>%\n  count(hometown, sort = TRUE)\n\n# Source:     SQL [8 x 2]\n# Database:   sqlite 3.39.1 [/Volumes/Essential/Study/Private/Writing/Distill/_posts/2022-04-07-talk-with-database-using-tidyverse-part-i/data/great_brit_bakeoff.db]\n# Ordered by: desc(n)\n  hometown                              n\n  <chr>                             <int>\n1 Wigan                                 1\n2 West Molesey, Surrey                  1\n3 Ongar, Essex                          1\n4 Market Harborough, Leicestershire     1\n5 Leeds / Luton                         1\n6 Bradford                              1\n7 Barton-Upon-Humber, Lincolnshire      1\n8 Barton-Le-Clay, Bedfordshire          1\n\n이 결과에 따르면, 제빵대회 우승자들의 출신 지역은 각기 다르다고\n결론을 내릴 수 있겠네요.\ndplyr 문법을 SQL 쿼리문으로\n앞서 {dplyr}을 이용해 수행한 질의를 SQL 쿼리문으로는\n어떻게 작성할까요? 코드 한 줄이면 손쉽게 알 수 있습니다.😀\n\n\ntbl(con, \"baker_results\") %>% \n  select(series, baker, hometown, series_winner) %>% \n  filter(series_winner == 1) %>% \n  count(hometown, sort = TRUE) %>% \n  show_query()\n\n<SQL>\nSELECT `hometown`, COUNT(*) AS `n`\nFROM (\n  SELECT `series`, `baker`, `hometown`, `series_winner`\n  FROM `baker_results`\n)\nWHERE (`series_winner` = 1.0)\nGROUP BY `hometown`\nORDER BY `n` DESC\n\n멋지지 않습니까? 이제 제가 왜 이 글의 맨 위 요약을 “R을 통해\n데이터베이스 관련 작업을 한다면, 꼭 SQL 전문가가 될 필요는 없어요!”라고\n적은 지 아시겠나요? {dplyr}로 작업을 수행하고, SQL\n쿼리문으로 변환을 수행해보는 작업은 SQL을 배우는 과정에 꽤 큰 도움이\n될겁니다. 직장 또는 기관에서 DB를 관리할 때 모두 같은 업체의 SQL DB를\n사용하는 건 아니므로, 이렇게 광범위한 업체들로부터 공급되는 SQL을 알고,\n읽는 것은 언제나 중요하기 때문입니다.\n출력문의\nlazy query / ??의 의미\n앞서 테이블, 쿼리를 작성하며 출력물에서\nSource: table [?? x 5] 또는\nSource: lazy query [?? x 4]와 같은 문장을 확인하실 수\n있었을 겁니다.\n이런 문장이 출력물에\n포함되는 이유\n먼저, 우리가 직접적인 RDBMS 상에서가 아닌 R이라는 공간을 빌려\n뒤에서(behind the scenes) 작성한 dplyr코드는 우리가\n연결하려는 DB의 SQL에 해당하는 dialect로 변환됩니다.\n즉, SQL은 DB에 직접적으로 실행됩니다. 즉, 데이터를 먼저 R로\n가져와서 조작하는 것이 아닌 쿼리 자체를 DB에 보내고 DB에서\n계산(computation)이 수행됩니다.\n정리하면, dplyr 파이프라인을 사용해 DB에서 쿼리를\n실행하면, DB에서 계산을 수행하고 실행된 최종 결과의 전체가 아닌 일부를\nR에서 보여주는 식입니다.\n이러한 이유들을 들여다보면 우리는 ??를 이해할 수\n있습니다.\n??는 “연결 DB con에서 이\n쿼리(파이프라인을 SQL로 변환시킨 것)를 실행했고, 여기 R에서 출력물을\n스니펫(snippet)으로만 가져왔는데, 얼마나 많은 수의 행이 있는지에 관한\n메타 정보까진 캐치하진 못했어. 그저 출력물에 몇 개의 열이 있다는 것\n정도만 캐치했어”라고 이해할 수 있습니다.\n이 튜토리얼은 파트 1 입니다. 다음 파트에서 가져온 테이블에 얼마나\n많은 행들이 존재하는 지와 같은 메타 정보들을 R로 어떻게 가져오는지에\n대해 알아볼 예정입니다.\n3 DB 연결 해제하기\n작업이 끝나면 연결을 해제하는 것을 잊지마세요!\n\n\ndbDisconnect(con) # db 연결 닫기\n\n\n연결 해제가 체크는 dbListTable(con)을 실행해보시면\n됩니다. 연결해제가 잘 되었다면 에러문이 출력될겁니다.\n다음 파트에서 배울 내용\ndplyr에 관해 좀 더 깊게(e.g. 테이블\n조인하기)\n데이터 R로 가져오기\n\n\n\nNaidoo, Vebash. 2020. “Sciencificity’s Blog: Using the Tidyverse\nwith Databases - Part i,” December. https://sciencificity-blog.netlify.app/posts/2020-12-12-using-the-tidyverse-with-databases/.\n\n\nrelational database management\nsystem, or RDBMS↩︎\nStructured Query Language↩︎\n공급업체마다 SQL 쿼리문에 조금씩\n차이가 있는 부분을 방언의 의미를 갖는 dialect로 표현한 것 같습니다.↩︎\n명확하게 정해진 바는 없어보이나,\nd-plier라고 흔히 읽는 듯 합니다. 패키지의 스티커 이미지를 보면\n플라이어가 그려져 있습니다.↩︎\nDataBase Interface, 데이터베이스\n인터페이스의 약어↩︎\n본 예제에서는 다루지 않지만, Rstudio DB에서 많은 것을 확인할 수\n있습니다.↩︎\n본 예제에서는 다루지 않지만, Rstudio DB에서 확인할 수 있습니다.↩︎\n",
    "preview": "posts/2022-04-07-talk-with-database-using-tidyverse-part-i/preview.jpg",
    "last_modified": "2022-09-13T22:40:30+09:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/",
    "title": "tidyvese principle로 머신러닝 하기",
    "description": "tidymodels ecosystem 소개",
    "author": [
      {
        "name": "Taemo Bang",
        "url": {}
      }
    ],
    "date": "2022-04-04",
    "categories": [
      "Statistical/Machine Learning",
      "R"
    ],
    "contents": "\n\nContents\n1 데이터\n분할: {rsample}\n2 데이터 전처리 및\nFeature Engineering: {recipes}\n3 모형 정의 및 적합:\n{parsnip}\n4\n적합된 모형 요약: {broom}\n5 모형 성능 평가: {yardstick}\n6 모형의 모수 튜닝: {tune},\n{dials}\n맺음말\n\n\n본 글은 (Plieninger,\nn.d.)를 기반으로 작성되었습니다.\ntidymodels ecosystem은 R에서 머신러닝을 tidyverse principle로 수행할\n수 있게끔 해주는 패키지 묶음입니다. 전처리, 시각화부터 모델링, 예측까지\n모든 과정을 “tidy” framework로 진행하게 해주죠. tidymodels은\n{caret}1을 완벽하게 대체하며, 더 빠르게\n그리고 더 직관적인 코드로 모델링을 수행할 수 있습니다.\n{tidymodels}는 모델링에 필요한 패키지들의 묶음이라고 보면\n됩니다. {tidyverse}처럼 {tidymodels}를\n로딩하면 모델링에 쓰이는 여러 패키지의 묶음을 불러와줍니다. 그중에는\n{ggplot2}와 {dplyr} 같은 {tidyverse}에 포함되는 패키지들도 있습니다.\n본격적으로 튜토리얼을 시작하기 전에 필요한 패키지와 데이터를 먼저\n불러오겠습니다.\n\n\nlibrary(tidymodels)\nlibrary(ggrepel) # for geom_label_repel()\nlibrary(corrplot) # for corrplot()\nggplot2::theme_set(theme_light())\n\n\n본 튜토리얼에서 이용할 toy data는\n`diamonds{ggplo2}`💎입니다. 해당 데이터는 다이아몬드의\n등급과 크기 및 가격에 관한 정보를 갖습니다:\n\n\ndata(diamonds)\nglimpse(diamonds)\n\nRows: 53,940\nColumns: 10\n$ carat   <dbl> 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22…\n$ cut     <ord> Ideal, Premium, Good, Premium, Good, Very Good, Very…\n$ color   <ord> E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J…\n$ clarity <ord> SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, …\n$ depth   <dbl> 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1…\n$ table   <dbl> 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, …\n$ price   <int> 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 33…\n$ x       <dbl> 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87…\n$ y       <dbl> 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78…\n$ z       <dbl> 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49…\n\n다음은 우리가 모델링에 사용할 features(\\(X\\))들의 상관계수 행렬을 시각화 한 것이며,\n상관계수 행렬을 다이아몬드의 가격(price, \\(y\\)) 열의 상관계수의 절댓값을 기준으로\n내림차순 정렬하여 그린 것입니다.\n\n\nset.seed(1)\ndiamonds %>% \n  sample_n(2000) %>% \n  mutate_if(is.factor, as.numeric) %>%\n  cor %>% \n  {.[order(abs(.[, \"price\"]), decreasing = TRUE),\n     order(abs(.[, \"price\"]), decreasing = TRUE)]} %>% \n  corrplot(method = \"number\", type = \"upper\", \n           mar = c(0, 0, 1.5, 0), tl.col = \"black\")\n\n\n\n\ntoy data를 이용해 {tidymodels}의 전반적인 진행 과정을\n보여주는 예제이기 때문에, 상관계수 행렬 그림은 전체 데이터가 아닌\n2,000개만을 샘플링하여 그렸습니다.\n1 데이터 분할: {rsample}\ntidymodels ecosystem을 구성하는 패키지들 중 가장 먼저 소개할 친구는\n데이터 분할에 쓰이는 {rsample}입니다. 본 예제의 마지막\n단계에서 시험 자료(test data)를 기반으로 모형의 예측 성능을 평가할\n것이기 때문에, 먼저 데이터를 훈련 자료(training data), 시험 자료로\n분할해야 합니다. 이번에도 모형 적합 및 교차 검증을 이용한 모수 튜닝\n단계에서의 계산 비용 절감을 위해, 훈련 자료의 비율을 10%로 낮게 잡아\n데이터를 나눌 것입니다. 다음의 모든 과정은 {rsample}\n패키지의 함수들로 진행됩니다. 패키지 또는 함수의 이름이 직관적이고 인간\n친화적이면 그 역할을 기억하기 쉬운데, 앞으로 소개할\n{tidymodels}를 구성하는 패키지와 패키지를 이루는 함수들의\n이름은 대부분 이러한 점을 고려하여 네이밍이 되어있습니다.😊\n\n\nset.seed(1)\ndia_split <- initial_split(diamonds, prop = .1, strata = price)\ndia_train <- training(dia_split)\ndia_test <- testing(dia_split)\ncat(\"the number of observations in the training set is \", \n    nrow(dia_train), \n    \".\\n\",\n     \"the number of observations in the test set is \", \n    nrow(dia_test), \".\", \n    sep = \"\")\n\n\nthe number of observations in the training set is 5393.\nthe number of observations in the test set is 48547.\n\n2 데이터 전처리 및\nFeature Engineering: {recipes}\n다음으로는 {recipes}를 이용하여, 데이터 전처리 및\nFeature Engineering을 수행한다. recipe는 요리법이라는 뜻뿐만 아니라 특정\n결과를 가져올 듯한 방안의 뜻2도 갖습니다. 이럴 때마다\n영어권의 R 유저들이 부럽습니다. 패키지나 함수 이름을 통해 그 역할을\n기억하고 필요할 때 꺼내쓰기가 좀 더 편하지 않을까 하는 생각이 드네요.\n{recipes}의 step_*() 함수들을 이용해 모델링에\n사용할 자료를 준비3할 수 있습니다. 다음의 산점도는\n다이아몬드의 가격(price)과 carat 사이에 비선형적인 관계가 있음을\n암시하며, 이러한 관계는 carat의 다항함수를 변수로 도입하여 모델링에\n반영할 수 있습니다.\n\n\nqplot(carat, price, data = dia_train) +\n  scale_y_continuous(trans = log_trans(), labels = function(x) round(x, -2)) +\n  geom_smooth(method = \"lm\", formula = \"y ~ poly(x, 4)\") +\n  labs(title = \"The degree of the polynomial is a potential tuning parameter\")\n\n\n\n\nrecipe()는 자료와 모형식을 인수로 하며,\nstep_*() 함수들을 이용하여 step by step👞으로 다양한\n전처리를 수행할 수 있게끔 해줍니다.4 여기서는 \\(y\\)에 로그 변환(step_log())을\n수행하고, 연속형 예측변수5에 표준화(중심화 및 척도화,\nstep_normalize()), 범주형 예측변수는 더미\n변수화(step_dummy())를 수행합니다. 그리고,\nstep_poly()를 이용해 carat의 2차 효과를 반영해주었습니다.\n준비가 끝난 recipe 객체는 prep() 함수를 통해 자료에 수행된\n전처리들을 확인할 수 있다.\n\n\ndia_rec <- recipe(price ~ ., data = dia_train) %>% \n  step_log(all_outcomes()) %>% \n  step_normalize(all_predictors(), -all_nominal()) %>% \n  step_dummy(all_nominal()) %>% \n  step_poly(carat, degree = 2)\nprep(dia_rec)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          9\n\nTraining data contained 5393 data points and no missing data.\n\nOperations:\n\nLog transformation on price [trained]\nCentering and scaling for carat, depth, table, x, y, z [trained]\nDummy variables from cut, color, clarity [trained]\nOrthogonal polynomials on carat [trained]\n\nrecipe 객체에 prep()를 적용한 것에\njuice()를 수행하면 전처리가 수행된 자료를 추출할 수\n있죠.\n\n\ndia_juiced <- juice(prep(dia_rec))\nglimpse(dia_juiced)\n\nRows: 5,393\nColumns: 25\n$ depth        <dbl> 0.52494063, -0.86779062, -0.51960781, 0.5945771…\n$ table        <dbl> -0.2037831, 1.5902131, 0.6932150, -0.2037831, -…\n$ x            <dbl> -1.5716610, -1.5805830, -1.2772351, -1.3218451,…\n$ y            <dbl> -1.6114895, -1.5845446, -1.2612061, -1.3061143,…\n$ z            <dbl> -1.5470415, -1.6483712, -1.3154309, -1.2575282,…\n$ price        <dbl> 5.872118, 5.877736, 6.003887, 6.003887, 6.31354…\n$ cut_1        <dbl> 3.162278e-01, -1.481950e-18, 6.324555e-01, -1.4…\n$ cut_2        <dbl> -0.2672612, -0.5345225, 0.5345225, -0.5345225, …\n$ cut_3        <dbl> -6.324555e-01, -3.893692e-16, 3.162278e-01, -3.…\n$ cut_4        <dbl> -0.4780914, 0.7171372, 0.1195229, 0.7171372, 0.…\n$ color_1      <dbl> 3.779645e-01, -5.669467e-01, 3.779645e-01, 3.77…\n$ color_2      <dbl> -5.621884e-17, 5.455447e-01, -5.621884e-17, -5.…\n$ color_3      <dbl> -4.082483e-01, -4.082483e-01, -4.082483e-01, -4…\n$ color_4      <dbl> -0.5640761, 0.2417469, -0.5640761, -0.5640761, …\n$ color_5      <dbl> -4.364358e-01, -1.091089e-01, -4.364358e-01, -4…\n$ color_6      <dbl> -0.19738551, 0.03289758, -0.19738551, -0.197385…\n$ clarity_1    <dbl> 0.07715167, -0.07715167, -0.38575837, -0.231455…\n$ clarity_2    <dbl> -0.38575837, -0.38575837, 0.07715167, -0.231455…\n$ clarity_3    <dbl> -0.1846372, 0.1846372, 0.3077287, 0.4308202, 0.…\n$ clarity_4    <dbl> 0.3626203, 0.3626203, -0.5237849, -0.1208734, -…\n$ clarity_5    <dbl> 0.3209704, -0.3209704, 0.4921546, -0.3637664, -…\n$ clarity_6    <dbl> -0.30772873, -0.30772873, -0.30772873, 0.553911…\n$ clarity_7    <dbl> -0.59744015, 0.59744015, 0.11948803, -0.3584640…\n$ carat_poly_1 <dbl> -0.01605633, -0.01634440, -0.01432792, -0.01432…\n$ carat_poly_2 <dbl> 0.017042209, 0.017792818, 0.012731517, 0.012731…\n\n또한, recipe 객체에 prep()를 적용한 것에\njuice()가 아닌 bake()를 수행하면 새로운 자료에\nrecipe 객체에 수행했던 것과 같은 전처리를 수행할 수 있습니다. 예를 들어,\n다음은 시험 자료에 대해 훈련 자료에 수행한 전처리를 수행한 뒤에 해당\n자료를 추출하라는 것과 같죠. 시험 자료의 예측을 통한 모형의 성능평가에는\n사전에 훈련자료와 동일한 전처리가 필요로되는데, bake()는\n이러한 시간을 크게 단축시켜줍니다.\n\n\nglimpse(\n  bake(prep(dia_rec), dia_test)\n)\n\nRows: 48,547\nColumns: 25\n$ depth        <dbl> -0.1714250, -1.3552466, -3.3747069, 0.4553041, …\n$ table        <dbl> -1.1007812, 1.5902131, 3.3842094, 0.2447160, 0.…\n$ x            <dbl> -1.589505, -1.643037, -1.500285, -1.366455, -1.…\n$ y            <dbl> -1.575563, -1.701306, -1.494728, -1.351022, -1.…\n$ z            <dbl> -1.604944, -1.778652, -1.778652, -1.315431, -1.…\n$ price        <dbl> 5.786897, 5.786897, 5.789960, 5.811141, 5.81413…\n$ cut_1        <dbl> 6.324555e-01, 3.162278e-01, -3.162278e-01, 3.16…\n$ cut_2        <dbl> 0.5345225, -0.2672612, -0.2672612, -0.2672612, …\n$ cut_3        <dbl> 3.162278e-01, -6.324555e-01, 6.324555e-01, -6.3…\n$ cut_4        <dbl> 0.1195229, -0.4780914, -0.4780914, -0.4780914, …\n$ color_1      <dbl> -0.3779645, -0.3779645, -0.3779645, 0.3779645, …\n$ color_2      <dbl> 8.914347e-17, 8.914347e-17, 8.914347e-17, -5.62…\n$ color_3      <dbl> 0.4082483, 0.4082483, 0.4082483, -0.4082483, 0.…\n$ color_4      <dbl> -0.5640761, -0.5640761, -0.5640761, -0.5640761,…\n$ color_5      <dbl> 0.4364358, 0.4364358, 0.4364358, -0.4364358, 0.…\n$ color_6      <dbl> -0.19738551, -0.19738551, -0.19738551, -0.19738…\n$ clarity_1    <dbl> -0.38575837, -0.23145502, 0.07715167, -0.077151…\n$ clarity_2    <dbl> 0.07715167, -0.23145502, -0.38575837, -0.385758…\n$ clarity_3    <dbl> 0.3077287, 0.4308202, -0.1846372, 0.1846372, 0.…\n$ clarity_4    <dbl> -0.5237849, -0.1208734, 0.3626203, 0.3626203, -…\n$ clarity_5    <dbl> 0.4921546, -0.3637664, 0.3209704, -0.3209704, 0…\n$ clarity_6    <dbl> -0.30772873, 0.55391171, -0.30772873, -0.307728…\n$ clarity_7    <dbl> 0.11948803, -0.35846409, -0.59744015, 0.5974401…\n$ carat_poly_1 <dbl> -0.01634440, -0.01692054, -0.01634440, -0.01461…\n$ carat_poly_2 <dbl> 0.01779282, 0.01932160, 0.01779282, 0.01342699,…\n\n3 모형 정의 및 적합: {parsnip}\n이제 훈련 자료에 대한 기본적인 전처리가 끝났으므로,\n{parsnip}을 이용하여 모형을 정의하고 적합하려고 합니다.\n{parsnip}은 우리나라 말로 연노란색의 긴 뿌리채소를\n뜻하는데, 왜 이렇게 네이밍이 된 지는 아직 잘 모르겠습니다. 영어권의\n원어민들은 어떻게 생각할지 궁금하네요. {parsnip}은 인기\n있는 수많은 머신러닝 알고리즘6을 제공해줍니다. 그리고,\n최대 장점은 단일화된 인터페이스로 여러 모형을 적합할 수 있다는 점이죠.\n예를 들어, 랜덤포레스트를 제공하는 두 패키지 {ranger}와\n{randomForest}에는 고려할 트리의 개수를 지정하는 모수가\n존재하는데 해당 옵션의 이름이 각각 ntree,\nnum.trees로 다릅니다. 이는 사용자들에게 꽤 불편한 점일 수\n있는데, {parsnip}은 이러한 문제를 해결해줌으로써 두\n인터페이스를 모두 기억할 필요가 없게끔 해줍니다.\n{parsnip}에서는 먼저 특정 함수를 통해 모형을 정의하고7, set_mode()로 어떤\n문제8를 해결할 것인지 설정한 뒤에,\n마지막으로 어떤 시스템 또는 패키지를 이용하여 해당 모형을 적합할지를\nset_engine()으로 설정합니다. 여기서는 먼저\nstats::lm() 엔진을 이용하여 기본적인 회귀모형으로 적합을\n시작해 보겠습니다.\n\n\nlm_model <- linear_reg() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lm\")\n\n\n본격적인 모형 적합 전에, 앞서 언급했던 {parsnip}의\n장점을 확인해보기 위해 랜덤포레스트를 예로 들어보겠습니다. 랜덤포레스트\n모형의 적합에는 {ranger} 또는 {randomForest}를\n이용할 수 있는데, 서로 조금 다른 인터페이스를 지닌다고 했었습니다.\n{parsnip}은 다음과 같이 엔진 설정 전에\n{parsnip}만의 함수로 먼저 모형을 정의하고 해당 함수에서\n모수를 설정함으로써 서로 다른 인터페이스를 통합하여줍니다.\n\n\nrand_forest(mtry = 3, trees = 500, min_n = 5) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"ranger\", importance = \"impurity_corrected\")\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 3\n  trees = 500\n  min_n = 5\n\nEngine-Specific Arguments:\n  importance = impurity_corrected\n\nComputational engine: ranger \n\n이제 다시 회귀모형으로 돌아오겠습니다. 설정했던 기본적인 회귀모형을\n전처리를 완료한 훈련 자료에 적합해 줍니다.\n\n\nlm_fit1 <- fit(lm_model, price ~ ., dia_juiced)\nlm_fit1\n\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = price ~ ., data = data)\n\nCoefficients:\n (Intercept)         depth         table             x             y  \n   7.7110881     0.0582418     0.0138979     0.8357574     0.2337963  \n           z         cut_1         cut_2         cut_3         cut_4  \n   0.0532288     0.1134826    -0.0282144     0.0315527    -0.0020513  \n     color_1       color_2       color_3       color_4       color_5  \n  -0.4452258    -0.0887138    -0.0090620     0.0071217    -0.0059503  \n     color_6     clarity_1     clarity_2     clarity_3     clarity_4  \n  -0.0001745     0.9025208    -0.2480065     0.1424917    -0.0664178  \n   clarity_5     clarity_6     clarity_7  carat_poly_1  carat_poly_2  \n   0.0265924     0.0031308     0.0245773    -3.1129423    -6.9995161  \n\n예제에서 사용되진 않았지만, step_rm()을 이용하여 사전에\n모델링에 필요 없는 변수는 제거할 수도 있습니다.\n4 적합된 모형 요약: {broom}\nR에서 여러 모형 객체들의 요약은 summary() 또는\ncoef()와 같은 함수로 이루어집니다. 그러나, 이러한 함수들의\n출력물은 타이디한\n포맷9으로 주어지지 않습니다.\n{broom} 패키지는 적합 된 모형의 요약을 타이디한 포맷으로\n제공해줍니다. broom은 빗자루와 같은 브러쉬를 의미하는 명사인데, 적합한\n모형을 깨끗하게 쓸어 담는 패키지라고 생각하면 기억하기 쉽지 않을까\n싶습니다. 이와 같이 패키지 이름, 함수 이름 하나하나를 신중하게\n네이밍하는 일관성은 {tidyverse}, {tidymodels}에 포함되는 패키지들의\n공통된 좋은 특징이라 할 수 있다. 실제로 R4DS10\n책에서도 Hadley Wickham은 객체의 이름이나 함수의 이름을 설정하는 것에\n있어서 어느정도의 시간을 투자하는 것은 전혀 아깝지 않다고 말하기도\n했습니다.\n{broom} 패키지를 구성하는 첫 번째 함수로\nglance()를 소개합니다. glance는 힐끗 본다는 뜻을 갖는다는\n점에서 추측할 수 있듯이, 적합된 모형의 전체적인 정보를 간략히\n제공해줍니다.\n\n\nglance(lm_fit1$fit)\n\n\n\n\n\n\n적합된 모형의 수정된 \\(R^2\\)\n값(adj.r.squared)은 약 98.27%로 상당히 높은 설명력을\n보여줍니다. RMSE는 sigma 열에서 확인할 수 있습니다.\n다음으로 tidy()는 추정된 모수에 대한 정보를 제공합니다.\n다음의 결과에서 우리는 carat의 2차 효과가 유의하게 존재함을\n알 수 있습니다. 통계량의 크기를 기준으로 내림차순으로 정렬하여\n표시하였습니다.\n\n\ntidy(lm_fit1) %>% \n    arrange(desc(abs(statistic)))\n\n\n\n\n\n\n마지막으로 augment()는 모형의 예측값, 적합값 등을\n반환해줍니다. augment는 우리나라 말로 어떤 것의 양 또는 값, 크기 등을\n늘리는 것11을 뜻하는 동사로, 해당 함수도 이름을\n통해 어느정도 그 역할을 가늠할 수 있죠.\n\n\nlm_predicted <- augment(lm_fit1$fit, data = dia_juiced) %>% \n  rowid_to_column()\nselect(lm_predicted, rowid, price, .fitted:.std.resid)\n\n\n\n\n\n\n앞서 생성한 lm_predicted 객체를 이용해 적합값과 관측값\n간의 산점도를 그려보았습니다. 잔차의 크기가 2 이상인 관측치에 대해서는\n해당 관측치의 행 번호를 붙여주었으며, 겹치는 점이 있는 경우를 고려하여\n점에 투명도를 주었습니다.\n\n\nggplot(lm_predicted, aes(.fitted, price)) +\n  geom_point(alpha = .2) +\n  ggrepel::geom_label_repel(aes(label = rowid),\n                            data = lm_predicted %>% filter(abs(.resid) > 2)) +\n  labs(x = \"fitted values\",\n       y = \"observed values\")\n\n\n\n\n원자료의 각 행을 의미하는 두 단어 관측값(observed values)과\n실제값(actual values)은 서로 통용되니 어떤 용어를 써도 문제가 없습니다.\n특히, 머신러닝에서는 이를 데이터포인트(data point)라고 표현하기도\n합니다. 3가지 용어 모두 통용되는 말이니 몰랐다면 알아둡시다. 모든\n학문에서 그렇겠지만 통계학에서는 특히 정확한 용어 정의가 중요하므로,\n비슷한 용어 또는 비슷한 듯 다른 용어들이 있다면 틈틈이 정리하는 습관을\n갖는 것이 좋다.\n5 모형 성능 평가: {yardstick}\n위에서 glance()를 통해 적합된 모형의 성능을 RMSE, \\(R^2\\)를 통해 힐끗 확인할 수 있었습니다.\n{yardstick}은 모형의 성능에 대한 여러 측도를 계산하기 위한\n패키지입니다. 물론, \\(y\\)가 연속형이든\n범주형이든 문제없으며 교차 검증(Cross Validation, CV)에서 생산되는\n그룹화된 예측값들과도 매끄럽게 잘 작동한다. yardstick은 기준, 척도를\n뜻하는 명사에 해당하므로, 기억하기도 쉬울 것이라 생각합니다. 이제는\n{rsample}, {parsnip},\n{yardstick}으로 교차 검증을 수행하여 좀 더 정확한 RMSE를\n추정해봅시다.\n다음 코드 블럭들에서 나타날 긴 파이프라인(pipeline,\n%>%)들을 정리해서 간략히 나타내면 다음과 같습니다.\n천천히 음미해보시기 바랍니다:\nrsample::vfold_cv()를 훈련용 자료를 3-fold CV를 수행할\n수 있도록 분할\nrsample::analysis()와\nrsample::assessment()를 이용해 각 분할에서 모형 훈련용,\n평가용 자료를 불러옴\n앞서 만든 모형 적합 전 전처리가 완료된 recipe 객체\ndia_rec을 각 fold의 모형 훈련용 자료에 prepped\n시킴\npreped한 훈련용 자료를 recipes::juice()로\n불러오고, recipes::bake()를 이용해 훈련용 자료에 처리한\n것과 같은 처리를 평가용 자료에 수행\nparsnip::fit()으로 3개의 모형 적합용(analysis) 자료\n각각에 모형을 적합(훈련)\npredicted()로 훈련시킨 각 모형으로 평가용(assessment)\n자료를 예측\n\n\nset.seed(1)\ndia_vfold <- vfold_cv(dia_train, v = 3, strata = price)\ndia_vfold\n\n\n\n\n\n\n\n\nlm_fit2 <- mutate(dia_vfold,\n                  df_ana = map(splits, analysis),\n                  df_ass = map(splits, assessment))\nlm_fit2\n\n\n\n\n\n\n\n\nlm_fit3 <- lm_fit2 %>% \n  mutate(\n    recipe = map(df_ana, ~prep(dia_rec, training = .x)),\n    df_ana = map(recipe, juice),\n    df_ass = map2(recipe,\n                  df_ass, ~bake(.x, new_data = .y))) %>% \n  mutate(\n    model_fit = map(df_ana, ~fit(lm_model, price ~ ., data = .x))) %>% \n  mutate(\n    model_pred = map2(model_fit, df_ass, ~predict(.x, new_data = .y)))\nselect(lm_fit3, id, recipe:model_pred)\n\n\n\n\n\n\n\n여기서 tidymodels ecosystem의 마법을 확인할 수 있습니다. 위 과정에서\n확인했다시피, 꽤 복잡한 과정들이 단 하나의 티블 객체\nlm_fit2에서 이루어졌습니다. 이렇게 복잡한 작업이 단 하나의\n티블 객체만으로 이루어질 수 있었던 이유는, 티블은\n리스트-열(list-column)을 가질 수 있기 때문이죠. 덕분에 우리는 R에서\n연산이 느린 반복문(e.g. for(), while())을\n사용하지 않고, purrr::map()을 loop로 이용하여 반복문을 통한\n지루하고 느린 모델링 작업을 완벽한 함수형 프로그래밍으로 수행할 수 있게\n되었습니다. R 사용자라면 어디서 한번 쯤은 반복문의 사용은 지양하고,\n함수형 프로그래밍을 해야 한다고 들어봤을 것입니다.\n{tidymodels}이 모델링 과정을 {tidyverse}와\n함께 작동할 수 있게 해줌으로써, 한 자료에 대해서 여러 가지 모형의 적합,\n교차검증을 통한 모수 튜닝, 예측 성능평가 등의 작업을 통해\n경험적으로(empirically) 최적의 모형을 선택하는 수고가 필요한 머신러닝에\n드는 시간을 상당히 줄여줬다고 할 수 있습니다.\n이쯤 되면 제가 왜 {tidyverse}를 좋아하고,\n{tidymodels}의 튜토리얼을 이렇게 상세하게 기술하는지\n이해하실 거라고 생각합니다. 이제 평가용 자료로부터 실제\n관측값(price)을 추출하여 예측값(.pred)과\n비교한 뒤, yardstick::metrics()를 이용해 여러 평가 측도를\n계산해보려고 합니다.\n\n\nlm_preds <- lm_fit3 %>% \n  mutate(res = map2(df_ass, model_pred, ~data.frame(price = .x$price, \n                                                    .pred = .y$.pred))) %>% \n  select(id, res) %>% \n  tidyr::unnest(res) %>% \n  group_by(id)\nlm_preds\n\n\n\n\n\n\n\n\nmetrics(lm_preds, truth = price, estimate = .pred)\n\n\n\n\n\n\n여기서 계산한 평가 측도의 값은 out-of-sample에 대한 성능이므로 모형\n적합값에 대해 평가 측도를 계산한 glance(lm_fit1$fit)의\n결과와 비교하여 보면 당연히 조금은 떨어지는 성능을 보입니다.\nmetrics()는 연속형 outcome(\\(y\\))에는 위와 같이 RMSE, \\(R^2\\), MAE를 기본적인 측도로 제공해줍니다.\n물론, 범주형 outcome에 대해서도 다른 기본적인 측도를 제공해주죠. 또한,\n하나의 측도만으로 비교하길 원한다면 rmse()와 같이 RMSE\n값만을 제공해주는 함수도 이용할 수 있으며, metric_set()을\n이용하면 원하는 metrics들을 직접 커스텀하여 정의할 수도 있습니다.\n3-fold CV를 통해 훈련 자료를 분할 및 전처리하고 예측값을 구하여\nRMSE를 계산하는 과정을 담은 앞선 코드블럭들은 {tidyverse},\n{tidymodels}에 익숙한 사람이라면 편하게 읽어나가실 수\n있을겁니다. 그러나, 코드가 매우 긴 것도 사실입니다. 사실, 위 코드블럭은\n다음 섹션에서 소개할 {tune} 패키지를 이용하면 다음과 같이\n단 몇 줄로 간결하게 코딩할 수 있습니다.\n\n\ncontrol <- control_resamples(save_pred = TRUE)\nset.seed(1)\nlm_fit4 <- fit_resamples(lm_model, dia_rec, dia_vfold, control = control)\nlm_fit4 %>% \n    pull(.metrics)\n\n\n\n\n[[1]]\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.143 Preprocessor1_Model1\n2 rsq     standard       0.980 Preprocessor1_Model1\n\n[[2]]\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.127 Preprocessor1_Model1\n2 rsq     standard       0.984 Preprocessor1_Model1\n\n[[3]]\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.130 Preprocessor1_Model1\n2 rsq     standard       0.984 Preprocessor1_Model1\n\n6 모형의 모수 튜닝: {tune},\n{dials}\ntune은 조정하다12 라는 뜻을 갖는 동사이며, 말 그대로\n{tune} 패키지는 모수를 튜닝(조율)하는(e.g. via grid search)\n함수들을 제공합니다. 그리고, 어떤 것을 조정하는 다이얼13을\n의미하는 이름을 갖는 {dials} 패키지는 {tune}을\n통해 튜닝할 모수들을 정하는 역할을 합니다. 즉, {tune}과\n{dials}는 대개 함께 쓰이는 패키지라고 보면 됩니다. 본\n예제에서는 랜덤포레스트 모형을 튜닝하는 과정을 보여줄 것입니다.\n6.1 튜닝을 위한 {parsnip}\n모형 객체 준비\n첫 번째로, 랜덤포레스트 모형을 형성할 때 매 트리 적합시 고려할\n변수들의 개수를 조정하는 mtry 모수를 조율해줍니다.\ntune()을 placeholder로 하여 후에 교차검증을 통해 최적의\nmtry를 선정할 입니다.\n다음 코드블럭의 출력물은 mtry의 기본 최솟값은 1이고\n최댓값은 자료에 의존함을 의미합니다. 어떤 자료를 다루느냐에 따라\nfeature의 수는 다르므로, 따로 지정하지 않는한 mtry의\n최댓값은 자료에 의존하게 됩니다.\n\n\nrf_model <- rand_forest(mtry = tune()) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"ranger\")\nparameters(rf_model)\n\nCollection of 1 parameters for tuning\n\n identifier type    object\n       mtry mtry nparam[?]\n\nModel parameters needing finalization:\n   # Randomly Selected Predictors ('mtry')\n\nSee `?dials::finalize` or `?dials::update.parameters` for more information.\n\nmtry()\n\n# Randomly Selected Predictors (quantitative)\nRange: [1, ?]\n\n아직 랜덤포레스트 모형의 적합에 쓰이는 모수 값을 결정하지 않았으므로\n모형을 훈련 자료에 적합할 준비가 된 상태가 아니라고 할 수 있습니다.\n그리고, mtry의 최댓값은 update()를 사용해\n원하는 값을 명시할 수도 있고, 또는 finalize()를 사용해 해당\n자료가 갖는 예측변수의 수로 지정할 수도 있죠.\n\n\nrf_model %>% \n  parameters() %>% \n  update(mtry = mtry(c(1L, 5L)))\n\nCollection of 1 parameters for tuning\n\n identifier type    object\n       mtry mtry nparam[+]\n\n\n\nrf_model %>% \n  parameters() %>% \n  finalize(x = juice(prep(dia_rec)) %>% select(-price)) %>% \n  pull(\"object\")\n\n[[1]]\n# Randomly Selected Predictors (quantitative)\nRange: [1, 24]\n\n6.2 튜닝을 위한 자료 준비:\n{recipes}\n두 번째로 튜닝하고 싶은 것은 carat의 다항식 차수입니다.\n2 데이터 전처리\n및 Feature Engineering: {recipes}의 그림에서 확인했듯이, 최대\n4차까지의 다항식이 자료에 잘 적합 될 수 있음을 알 수 있습니다. 그러나,\n우리는 모수 절약의 원칙(priciplt of parsimony)14을\n생각할 필요가 있고, 그에 따라 더 간단한 모형도 자료에 잘 적합 될 수\n있다는 가능성을 배제해서는 안됩니다. 그래서, carat의 다항식\n차수 또한 교차 검증을 통해 최대한 간단하면서 좋은 성능을 내는\ncarat의 차수를 찾을 것입니다.\n모형의 적합에서 각 모형이 갖는 고유한 초모수15와\n달리 예측변수 carat의 차수는 {recipe}를 통해\n새로운 레시피 객체를 만들어 튜닝이 진행됩니다. 그 과정은 초모수를\n튜닝했던 과정과 유사합니다. 다음과 같이 step_poly()에\ntune()을 사용하여 훈련 자료(dia_train())에\n대한 2번째 레시피 객체를 만들어 줍니다.\n\n\ndia_rec2 <- recipe(price ~ ., data = dia_train) %>% \n  step_log(all_outcomes()) %>% \n  step_normalize(all_predictors(), -all_nominal()) %>% \n  step_dummy(all_nominal()) %>% \n  step_poly(carat, degree = tune())\n\ndia_rec2 %>% \n  parameters() %>% \n  pull(\"object\")\n\n[[1]]\nPolynomial Degree (quantitative)\nRange: [1, 3]\n\n고려하는 다항식의 차수 범위가 기본값으로 설정하여 [1, 3]으로\n되어있는데, 이 부분은 다음 섹션에서 {workflows} 패키지를\n소개하며 개선할 부분이니 신경 쓰지 않으셔도 됩니다.\n6.3 모든 것을 결합하기:\n{workflows}\nworkflow를 직역하면 어떤 작업의 흐름을 뜻하듯이,\n{workflows} 패키지는 recipe나\nmodel 객체와 같은 머신러닝 파이프라인의 다른 부분이라 할 수\n있는 것들을 한 번에 묶어주는 역할을 합니다.\n이를 위해서는 먼저 workflow()를 선언하여 객체를 만들고,\n6.2 튜닝을 위한 자료 준비:\n{recipes}에서 만든 recipe 객체와 6.1 튜닝을 위한 {parsnip}\n모형 객체 준비에서 만든 랜덤포레스트 모형 객체를\nadd_*()로 결합해줍니다.\n\n\nrf_wflow <- workflow() %>% \n  add_model(rf_model) %>% \n  add_recipe(dia_rec2)\nrf_wflow\n\n══ Workflow ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ──────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_log()\n• step_normalize()\n• step_dummy()\n• step_poly()\n\n── Model ─────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n\nComputational engine: ranger \n\n아직 mtry의 최댓값이 알려져있지 않고\ndegree의 최댓값이 기본 설정인 3으로 설정되어 있으므로, 두\n번째로는 rf_wflow 객체의 모수 설정을\nupdate()로 갱신할 것입니다.\n\n\nrf_param <- rf_wflow %>% \n  parameters() %>% \n  update(mtry = mtry(range = c(3L, 5L)),\n         degree = degree_int(range = c(2L, 4L)))\nrf_param %>% pull(\"object\")\n\n[[1]]\n# Randomly Selected Predictors (quantitative)\nRange: [3, 5]\n\n[[2]]\nPolynomial Degree (quantitative)\nRange: [2, 4]\n\n앞서 말했듯이 교차검증을 통해 튜닝을 수행할 것이기 때문에, 세\n번째로는 설정한 모수들의 조합을 만들어야 합니다. 복잡한 튜닝 문제에는\ntune_bayes()를 통한 베이지안\n최적화(Bayesian optimization)(Silge and\nJulia, n.d.)가 추천되지만, 해당 예제에서 고려하는 초모수들의\n조합 정도는 grid search로도 충분해 보입니다. 다음과 같이 필요로 되는\n모든 모수 조합의 grid를 만듭니다.\n\n\nrf_grid <- grid_regular(rf_param, levels = 3)\nrf_grid\n\n\n\n\nrf_grid <- grid_regular(rf_param, levels = 3)\nrf_grid %>% \n    paged_table()\n\n\n\n\n여기서 levels는 grid를 만드는 데 사용되는 각 모수의 수에 대한\n정숫값을 조정하는 옵션입니다. default 값이 levels = 3이므로 해당 옵션은\n생략해도 문제없을 것입니다. 교차 검증을 통한 모수 튜닝에는 수많은 모형을\n적합해야 하는데, 이 예제에서는 9개의 모수 집합과 3개의 folds를\n사용하므로 총 \\(3 \\times 9 = 27\\)개의\n모형을 적합해야 한다. 27개의 모형을 빠르게 적합하기 위해 병렬처리를\n수행하려고 합니다. 이는 {tune} 패키지에서 직접적으로\n지원받을 수 있습니다.\n\n\nlibrary(doFuture)\nall_cores <- parallel::detectCores(logical = FALSE) - 1\n\nregisterDoFuture()\ncl <- parallel::makeCluster(all_cores)\nplan(future::cluster, workers = cl)\n\n\n이제 튜닝을 시작합니다.\n\n\noptions(future.rng.onMisue = \"ignore\")\nrf_search <- tune_grid(rf_wflow, grid = rf_grid, resamples = dia_vfold,\n                       param_info = rf_param)\n\n\n\n튜닝 결과는 autoplot()과 show_best()로\n검토할 수 있습니다:\n\n\nautoplot(rf_search, metric = \"rmse\")\n\n\n\n\\(x\\) 축은 mtry를\n나타내며, 각 선의 색상은 고려한 다항식 차수를 나타냅니다.\nmtry는 5와 carat의 2차항까지 고려한 초모수 조합이 최적임을\n알 수 있습니다. show_best()로도 확인할 수 있습니다:\n\n\nshow_best(rf_search, \"rmse\", n = 9)\n\n\n\n\n\n\n\n\nselect_best(rf_search, metric = \"rmse\")\n\n\n\n\n\n\n그리고, select_by_one_std_err()을 이용하면 원하는 metric\n값의 \\(\\pm 1SE\\)를 고려한 최적의 초모수\n조합을 얻을 수도 있죠.\n\n\nselect_by_one_std_err(rf_search, mtry, degree, metric = \"rmse\")\n\n\n\n\n\n\n6.4 선택한 최적의 모형으로\n예측 수행\n6.3 모든 것을 결합하기:\n{workflows}에서 carat 변수는 2차항으로도 충분히 설명되고, 매 트리\n적합 시 고려할 변수의 수는 5개임을 확인할 수 있었습니다. 이제는 해당\n초모수 조합을 이용해 훈련 자료에 모형을 적합하고 최종 예측을 수행하려고\n합니다. 이번 예제에서는 설정값이 똑같긴 하지만, \\(\\pm 1SE\\)를 고려한 초모수 조합을 모형\n적합에 사용하였습니다.\n\n\nrf_param_final <- select_by_one_std_err(rf_search, mtry, degree, metric = \"rmse\")\nrf_wflow_final <- finalize_workflow(rf_wflow, rf_param_final)\nrf_wflow_final_fit <- fit(rf_wflow_final, data = dia_train)\n\n\n\n이제 적합된 모형객체 rf_wflow_final_fit으로 원하는\nunobserved 자료16를 predict()로 예측할\n수 있다. 우리에게는 미리 나눠둔 시험 자료 dia_test가\n있습니다. 다만, dia_test의 \\(y\\)는 로그변환이 취해지지 않았으므로,\npredict(rf_wflow_final_fit, new_data = dia_test)가 아닌\n{recipe}로 step_log()를 취해주어야 합니다.\n여기서는 workflow로부터 추출한 prepped된 recipe 객체를\n이용해 시험 자료에 대하여 bake()를 취할 것입니다. 그리고,\nbaked된 시험 자료를 적합한 최종 모형을 통해 예측할하면 되죠.\nbake()가 이렇게나 편합니다:\n\n\ndia_rec3 <- pull_workflow_prepped_recipe(rf_wflow_final_fit)\nrf_final_fit <- pull_workflow_fit(rf_wflow_final_fit)\n\ndia_test$.pred <- predict(rf_final_fit,\n                          new_data = bake(dia_rec3, dia_test)) %>% pull(.pred)\ndia_test$logprice <- log(dia_test$price)\n\nmetrics(dia_test, truth = logprice, estimate = .pred)\n\n\n\n\n\n\n시험 자료에 대한 RMSE는 약 0.11로 교차 검증에서 계산된 RMSE보다는\n조금 더 나은 성능을 보여줍니다.\n맺음말\n{tidymodels}의 ecosystem은 머신러닝 문제를 풀기 위해\n필요한 첫 단계부터 끝까지 함께 작동하는 패키지들의 집합을 한대 묶어\n제공해줍니다. 또한, {tidyverse}를 통한 data-wrangling\n기능과 훌륭한 시각화 패키지 {ggplot2}와도 함께 작동하는\n{tidymodels}은 R을 사용하는 데이터 사이언티스트들에게는\n더없이 풍부한 toolbox라 할 수 있을 것 같습니다. 아울러, 해당\n튜토리얼에서는 예측 모형들을 결합해주는17\n기능을 갖는 패키지 {stacks}에 대한 내용을 다루지 않았는데18, {tidymodels}을 불러올\n때 로딩이 되는 패키지는 아니지만, {stacks} 또한\n{tidymodels}의 한 부분으로 소개되는 패키지에 해당합니다. 그리고,\ntidymodels ecosystem을 “머신러닝”에만 국한시키기에는 너무나도 많은\n기능들이 업데이트되고 있습니다. 최근엔 반복측정자료분석에 자주 쓰이는\n모형 중 하나인 혼합효과모형(linear mixed model)까지 지원하기\n시작했습니다:\n\nLots of new #rstats package versions! Here’s a summary for the parsnip packages, including the new {multilevelmod} package!https://t.co/rv5Z9izpho— Max Kuhn (@topepos) March 24, 2022\n\n\ntidyverse 블로그를 꼭\n팔로우업하세요. 본 튜토리얼은 20년 2월에 작성된 글을 기반으로 쓰여졌기\n때문에 최신이라고 하긴 어렵습니다.😂 그러나, tidymodels ecosystem의\n기본기를 익히기에는 충분할 겁니다.\n이 튜토리얼이 {tidymodels}을 배우길 원하는, R로\n머신러닝을 수행하길 원하는 우리나라 R 유저들에게 조금이나마 도움이\n됐으면 좋겠습니다.😊 마지막으로 이 튜토리얼을 진행하는 데 쓰인 하드웨어\n정보를 제공하는 것을 끝으로 이 글을 마무리합니다.\n\n\nsessioninfo::session_info(info = \"platform\")\n\n─ Session info ─────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23)\n os       macOS Monterey 12.6\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Asia/Seoul\n date     2022-09-19\n pandoc   2.18 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n\n────────────────────────────────────────────────────────────────────\n\n\n\n\nPlieninger, Hansjörg. n.d. “Tutorial on Tidymodels for Machine\nLearning.” https://hansjoerg.me/2020/02/09/tidymodels-for-machine-learning/.\n\n\nSilge, Max Kuhn, and Julia. n.d. 14 Iterative Search | Tidy Modeling\nwith r. https://www.tmwr.org/iterative-search.html#bayesian-optimization.\n\n\nR에서 머신러닝을 수행할 때 주로\n사용되었던 패키지↩︎\na method or an idea that seems likely\nto have a particular result↩︎\n이른바, “전처리”(preprocessing)라고\n표현하기도 합니다.↩︎\nsee, e.g. `vignette(“Simple_Example”,\npackage = “recipes”)`↩︎\n예측변수는 predictor를 말하며,\nfeatures와 동의어라 볼 수 있음↩︎\nsee, For\na list of models available via parsnip↩︎\ne.g. linear_reg(),\nrand_forest()↩︎\nregression 또는 classification↩︎\n일반적으로는 행이 관측치 열이 변수인\n포맷을 말하나, 이러한 형식이 데이터를 다루는 것에 있어서 항상 정답이라는\n것은 아님↩︎\n원서↩︎\nto increase the amount, value, size,\netc. of something↩︎\nto make changes to an engine so that\nit runs smoothly and as well as possible↩︎\nthe round control on a radio,\ncooker, etc. that you turn in order to change something↩︎\n또는 Occam’s razor↩︎\n모수라는 뜻을 갖는 parameters보다는\n모형 적합 사전에 미리 설정이 필요로 되는 모수를 뜻하는 초모수의 뜻을\n갖는 hyperparameters라는 용어가 더 정확할 것이다↩︎\n모형 적합에 쓰이지 않은 자료↩︎\ne.g. ensemble, stacking, super\nlearner↩︎\n관심있는 분들은 see here↩︎\n",
    "preview": "posts/2022-04-04-do-machine-learning-with-tidyverse-principle/preview.jpg",
    "last_modified": "2022-09-19T22:34:49+09:00",
    "input_file": "tidyvese-principle.knit.md"
  },
  {
    "path": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/",
    "title": "tidyverse principle로 시계열 자료 분석하기",
    "description": "tidyverts ecosystem 소개",
    "author": [
      {
        "name": "Taemo Bang",
        "url": {}
      }
    ],
    "date": "2022-03-11",
    "categories": [
      "Time Series",
      "R"
    ],
    "contents": "\n\nContents\n1\ntsibble\n2 tsibbledata\n3 feasts\n4 fable\n5\nfable.prophet\n맺음말\n\n\n본 글은 (“Tidyverts,”\nn.d.)을 기반으로 작성되었습니다.\ntidyverts ecosystem은 시계열 자료에 관한 분석을 tidyverse principle로\n수행할 수 있게끔 해주는 패키지 묶음입니다. 전처리, 시각화부터 모델링,\n예측까지 모든 과정을 “tidy” framework로 진행하게 해주죠. tidyverse\npriciple이 데이터 전처리에 있어서 얼마나 많은 업무 생산성을 가져다\n주는지 우리는 이미 알고있습니다. 시계열 자료를 자주 다루는 사람이라면 꼭\n배워둘 만한 패키지죠.😄 tidyverts ecosystem을 이루는 대부분의 패키지들은\n{fpp3}으로 불러올 수 있습니다. {tsibbletalk}은\n{shiny}와 함께 동작하는 반응형 그래픽을 제공하는 패키지로\n본 튜토리얼에서는 생략하겠습니다:\n\n\nlibrary(fpp3)\nlibrary(fable.prophet)\nlibrary(nycflights13) # for nycflights13 data\nlibrary(purrr) # for map()\nggplot2::theme_set(theme_minimal())\nloaded_package <- c(\"fpp3\", \"fable.prophet\", \"nycflights13\", \"purrr\")\n.version <- map(loaded_package, packageVersion)\nnames(.version) <- loaded_package\n.version\n\n$fpp3\n[1] '0.4.0'\n\n$fable.prophet\n[1] '0.1.0'\n\n$nycflights13\n[1] '1.0.2'\n\n$purrr\n[1] '0.3.4'\n\n위 패키지들이 설치되어 있지 않은 분들은 튜토리얼의 본격적인 시작전에,\ninstall.packages(\"패키지명\")을 통해 설치해주시기 바랍니다.\n개발 버전을 설치하고 싶으신 분이 있다면 다음의 코드를 이용하세요:\n\n\n# install.packages(\"remotes\")\nremotes::install_github(\"tidyverts/tsibble\")\n\n\n1 tsibble\n1.1 Get Started\n{tsibble}은 일반적인 시계열 자료를 tibble\n형태로 표현할 수 있게해줍니다. 우리는 tsibble()을 통해 tidy한\n자료에 대해 수행해왔던 {tidyverse}를 이용한 wrangling을 수행할 수\n있습니다. 즉, tidyverse ecosystem이 tibble 객체를 기반으로 동작하듯이,\ntidyverts ecosytem은 tsibble 객체를 기반으로 동작합니다. tsibble 객체가\n갖는 기본적인 원칙은 다음과 같습니다:\nindex: 과거부터 현재까지 순서화된 자료값의 관측\n시간\nkey: 시간에 따른 관측 단위를 정의하는 변수의 집합\n각 관측치는 index와 key를 통해\n유일하게(uniquely) 식별되어야만 함\n각 관측치는 등간격으로 관측된 자료여야만 함\n즉, 티블(데이터프레임)을 tsibble로 변환하기(coerce)\n위해서는 key와 index를 명시해주어야 합니다.\n예를 들어, 다음과 같은 {nycflights13} 패키지의\nweather 자료를 이용해보겠습니다:\n\n\nweather_simple <- nycflights13::weather %>% \n    select(origin, time_hour, temp, humid, precip)\nweather_simple\n\n\n\n\n\n\norigin을 key로 index를\ntime_hour로 해주면 될 것 같습니다:\n\n\nweather_tsbl <- as_tsibble(weather_simple, key = origin, index = time_hour)\nweather_tsbl\n\n\n\n\n\n\n여기서는 자료 자체가 출발지(origin) 별로 기록된\n다중(multiple) 시계열에 해당하므로, key를\norigin으로 잡아줬지만, 만약 자료가 단일(univariate)\n시계열에 해당한다면 해당 key는 설정을 하지 않으면 됩니다(see\npackage?tsibble and vignette(\"intro-tsibble\")\nfor details). 그리고, 사실 tsibble()은 irregular time\ninterval을 갖는 자료에 대해서도 적용이 가능합니다. as_tsibble은\nregular = TRUE 옵션이 default로 설정되는데, 이를\nFALSE로 바꿔주면 되며, 이러한 irregular time interval을\n갖는 tsibble 객체의 경우는 [!] 표시를 통해 확인할 수\n있습니다:\n\n\nnycflights13::flights %>%\n    mutate(\n      sched_dep_datetime = make_datetime(year, month, day, hour, minute, \n                                         tz = \"America/New_York\")) %>%\n    as_tsibble(\n        key = c(carrier, flight), \n        index = sched_dep_datetime, \n        regular = FALSE\n        )\n\n\n\n\n\n\n1.2\nTurn impicit missing values into explicit missing values\n간혹 시계열 자료에는 암묵적 결측치(implicit missing values)가\n존재하는 경우가 있습니다. 암묵적 결측치가 존재하는 시계열 자료가 일정한\n시간 간격으로 수집되었을 경우, 우리는 fill_gaps()를 이용해\n암묵적 결측을 명시적으로(explicit) 바꿀 수 있어요. 4년간 수집된 연도별\n키위, 체리의 수확량(단위: kg)에 관한 자료를 직접 만들어서\nfill_gaps()의 쓰임에 대해 알아봅시다. 본 자료에는 암묵적\n결측이 존재합니다:\n\n\nharvest <- tsibble(\n    year = c(2010, 2011, 2013, 2011, 2012, 2014),\n    fruit = rep(c(\"kiwi\", \"cherry\"), each = 3),\n    kilo = sample(1:10, size = 6),\n    key = fruit, index = year\n)\nharvest\n\n\n\n\n\n\n암묵적 결측이란, 예를 들어 위 자료처럼 체리 생산량이 2010년에는\n기록되지 않았음에도 불구하고 행이 생략되어있는 것을 말합니다.\nNA로 명시는 다음과 같이 손쉽게 가능합니다:\n\n\nfill_gaps(harvest, .full = TRUE)\n\n\n\n\n\n\n다음의 각각 시작점, 끝점에 대해서만 결측치를 명시할 수도\n있습니다:\n\n\n# at the same starting point across units\nfill_gaps(harvest, .full = start())\n# at the same end point across units\nfill_gaps(harvest, .full = end())\n\n\n\n\n\n\n\n\n.full = FALSE를 설정할 경우(fill_gaps()의\ndefault 옵션에 해당), 각 key 내의 period에서 발생한 결측에 대해서만\n명시가 이루어집니다.\n\n\nfill_gaps(harvest, .full = FALSE)\n\n\n\n\n\n\n특정값으로의 명시도 손쉽게 수행이 가능해요.\n\n\nharvest %>% \n    fill_gaps(kilo = 0L)\n\n\n\n\n\n\n변수에 대해 함수를 적용하여 명시도 가능합니다. sum()을\n이용하여 합으로 명시해보았습니다:\n\n\nharvest %>%\n    fill_gaps(kilo = sum(kilo))\n\n\n\n\n\n\nkey에 대해 group_by를 통해 각 그룹에 대해\n함수를 적용할 수도 있죠. 이번에는 median()을 통해 중위수로\n명시해보았습니다:\n\n\nharvest %>%\n    group_by_key() %>%\n    fill_gaps(kilo = median(kilo))\n\n\n\n\n\n\n원 자료 자체에 NA가 존재하는 경우, 적용하고자 하는\n함수에 na.rm = TRUE을 설정해주면 됩니다:\n\n\nharvest[2, 3] <- NA\nharvest %>%\n    group_by_key() %>%\n    fill_gaps(kilo = median(kilo, na.rm = TRUE))\n\n\n\n\n\n\n마지막으로, fill_gaps()아 tidyr::fill()을\n함께 이용하면 암묵적 결측치를 이전 시점의 결측치로 대치할 수\n있습니다.\n\n\nharvest <- tsibble(\n    year = c(2010, 2011, 2013, 2011, 2012, 2014),\n    fruit = rep(c(\"kiwi\", \"cherry\"), each = 3),\n    kilo = sample(1:10, size = 6),\n    key = fruit, index = year\n)\nharvest %>%\n    group_by_key() %>%\n    fill_gaps() %>%\n    tidyr::fill(kilo, .direction = \"down\")\n\n\n\n\n\n\n반대로, 한 시점 미래의 값으로 대치도 가능합니다.\n\n\nharvest %>%\n    group_by_key() %>%\n    fill_gaps() %>%\n    tidyr::fill(kilo, .direction = \"up\")\n\n\n\n\n\n\n1.3 Aggregate over calendar periods\nindex_by()와 summarise()를 이용하면\n관심있는 변수에 대해 특정 시간 주기(e.g. monthly)에 대해 함수(e.g. 합계:\nsum(), 평균: mean())를 적용할 수 있어요.\nindex_by는 as.Date(),\ntsibble::yearweek(), tsibble::yearmonth(),\ntsibble::yearquarter(), 뿐만 아니라 {lubridate} 계열의\n함수와 함께 사용됩니다. 예를 들어, weather 자료의 월별 평균\n기온, 총 강수량은 다음과 같이 yearmonth()에\nindex 변수를 .으로 나타내어 계산할 수\n있습니다.\n\n\nweather_tsbl %>% \n    group_by_key() %>% \n    index_by(year_month = ~yearmonth(.)) %>%\n    summarise(\n        avg_temp = mean(temp, na.rm = TRUE),\n        total_precip = sum(precip, na.rm = TRUE)\n    )\n\n\n\n\n\n\nindex_by()+summarise()는 irregular time\ninterval을 갖는 tsibble에 대해서도 수행이 가능합니다.\n\n2 tsibbledata\n{tsibbledata}는 tsibble 형태의 다양한 예제 자료를 제공해줍니다. 어떤\n패키지에 대한 튜토리얼을 진행할 때, 적절한 자료들이 필요로 되는데,\n이렇게 예제 자료를 직접적으로 제공해준다는 점에서 R 유저들에 대한 배려가\n담겨있다는 생각이 드네요. 예를 들어, 다음의\nolympic_running은 4년 주기로 수집된 올림픽 달리기 종목의\n성별 최고기록에 관한 자료입니다(see ?olympic_running for\ndetails).\n\n\nolympic_running\n\n\n\n\n\n\n이 자료를 이용하여 달리기 종목별 최고 기록에 대한 시도표를 성별로\n나누어서 그려보았습니다. 참고로, 1916, 1940, 1944년의 경우 세계대전으로\n인해 결측 처리되었습니다.\n\n\nggplot(olympic_running, aes(x = Year, y = Time, colour = Sex)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(~ Length, scales = \"free_y\", nrow = 2) + \n  theme_minimal() + \n  scale_color_brewer(palette = \"Dark2\") + \n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  ylab(\"Running time (seconds)\")\n\n\n\n\n3 feasts\n{feasts}는 Feature Extraction And Statistics for Time Series의\n약자로, 시계열 자료분석에 쓰이는 여러가지 툴을 제공해줍니다. tsibble\n객체와 함께 동작하며, 시계열의 분해, feature 추출(e.g. 추세, 계절성),\n시각화 등을 수행할 때 쓰입니다. 아울러, {feasts}를 통한 시계열\n자료분석은 다음 섹션에서 소개할 tidyverts ecosystem의 예측 모델링 부분을\n담당하는 {fable} 패키지와 긴밀하게 결합하여 사용됩니다.\n3.1 Graphics\n시각화는 주로 시계열 자료의 패턴을 이해하기 위한 첫 단계에 많이\n이루어집니다. {feasts}는 시계열의 패턴을 {ggplot2}를 사용해 자유롭게\n커스텀할 수 있는 그래픽을 제공합니다. 첫 번째로는\ngg_season을 이용한 계절성(seasonality) 시각화입니다.\n시각화에 사용된 자료 tsibbledata::aus_production은 호주의\n맥주, 담배 등의 품목에 관한 분기별 생산지표 추정치에 관한 자료입니다.\n맥주의 분기별 생산지표에 관한 계절성 시각화를 수행해보았습니다:\n\n\naus_production %>% \n  gg_season(Beer)\n\n\n\n다음으로 gg_subseries()를 이용하면 시계열의 각\nseason별로 시각화가 가능합니다. 예를 들어, aus_production과\n같은 분기별 자료의 경우 분기별 패턴에 대한 시각화를 쉽게 수행할 수\n있습니다:\n\n\naus_production %>% \n  gg_subseries(Beer)\n\n\n\ngg_lag()를 이용하면 원자료와 시차(lag)의 산점도를\nseason별로 나누어 그릴 수 있습니다:\n\n\naus_production %>% \n  filter(year(Quarter) > 1991) %>% \n  gg_lag(Beer, geom = \"point\")\n\n\n\n분기별 자료의 특성상, lag 4와 8 그림을 보면 각 season별로 원자료와의\n관계가 \\(y=x\\) 직선에 잘 놓여있는 것을\n캐치할 수 있죠. 마지막으로 ACF 그림도 손쉽게 그릴 수 있습니다:\n\n\naus_production %>% \n  ACF(Beer) %>% \n  autoplot()\n\n\n\n3.2 Decompositions\n시계열 분해(decomposition)는 시계열 자료분석에서 흔히 수행되는 작업\n중 하나이며, 이는 시계열에 대한 패턴을 이해하는데에 큰 도움을 줍니다.\n그리고, 추후 예측 모델링을 정교하게 하는 것에도 상당한 도움을 준다. 즉,\n시계열 분해는 본인이 분석하고자 하는 시계열의 패턴을 좀 더 정교하게\n캐치하고 예측 성능을 향상시키기 위한 목적으로 꼭 필요로 되는 사전\n작업이라고 할 수 있습니다. 본 튜토리얼에서는 {feasts}에서 제공하고 있는\n2가지 시계열 분해 방법에 대해 소개하려고 합니다.\n3.2.1 Classical decompostion\nclassical decompostion은 1920년대에 고안된 방법입니다. 오래된\n방법론인 만큼 요즘 쓰이는 시계열 분해 방법들의 초석이 되는 방법이라고 할\n수 있으며, 다른 방법들에 비해 상대적으로 간단하다는 장점이 있습니다.\nclassical decompostion은 가법 분해와 승법 분해가 있습니다. 두 방법은\n계절성의 반영 방식에 따라 나뉩니다(e.g. 분기별 자료 \\(m = 4\\), 월별 자료 \\(m = 12\\), 일별 자료 \\(m = 7\\)). 보통 가법 classical\ndecompostion의 경우 계절성이 추세에 따라 무관하게 일정한 크기를 유지할\n때 사용하며, 반대로 계절성의 크기가 추세의 크기에 따라 변화하는 경우에는\n승법 classical decompostion을 사용합니다. 승법 계절성 classical\ndecompostion는 계절 성분이 연도에 따라 상수라고 가정한채로 진행되며,\n승법 계절성에서 계절 성분을 형성하는 \\(m\\)은 계절 지수(seasonal indices)라\n불리기도 합니다.\nclassical decompostion의 자세한 분해 과정은 여기를\n참고해주시기 바랍니다. 여기서는 바로 R을 이용한 튜토리얼을\n진행하겠습니다. 앞서 사용했던 자료의 맥주 생산지표를 가법 classical\ndecomposition을 통해 분해해보겠습니다.\n\n\ndcmp <- aus_production %>%\n    model(classical_decomposition(Beer, type = \"additive\"))\ncomponents(dcmp)\n\n\n\n\n\n\n먼저, 분해된 시계열의 요소들은 componenets()로 불러올 수\n있습니다. 그리고, 이 components()에 대해\nautoplot()을 수행해주면 다음과 같이 시각화를 수행할 수\n있습니다:\n\n\ndcmp %>%\n    components() %>% \n    autoplot() +\n    labs(title = \"Classical additive decomposition of Quarterly production of beer in Australia\")\n\n\n\n3.2.2 STL decomposition\nSTL은 “Seasonal and Trend decomposition using Loess”의 준말로\n다재다능(versatile)하고 로버스트한 시계열 분해 방법에 해당합니다.\n그리고, 여기서 loess란 Local\nregression의 준말로 자료를 비선형으로 추정하는 방법 중 하나에\n해당합니다. STL은 앞서 소개한 classical decomposition, 그리고\n{feasts}에서 제공하는 또 다른 시계열 분해 방법 SEATS,\nX-11과 비교하여 몇몇 이점을 갖는다. 자세한 사항은 여기를 참고해주세요. 본 글은\ntidyverts ecosystem에 대한 소개 이므로, deep한 이론 정리는 추후에 fpp3\n책을 공부하면서 하나하나 정리해나가겠습니다. 일단 바로 실습으로\n넘어가겠습니다.😊 다음은 STL decomposition을 이용하여 시계열의 추세\n요소는 window = 7을 통해 좀 더 flexible하게 추정하고, 계절\n패턴의 경우는 window = \"periodic\"으로 하여\n고정(fixed)되도록 하였습니다(see ?STL for details). 여기서.\nwindow란, 창을 말하며 자료를 여러 창으로 잘게 쪼갤수록 더\nflexible하고 복잡한 함수를 추정하게 됩니다. splines에\n지식이 있는 분들은 이해하기 쉬울거라고 생각합니다.\n\n\naus_production %>%\n  model(\n    STL(Beer ~ trend(window = 7) + season(window = \"periodic\"),\n        robust = TRUE)) %>%\n  components() %>%\n  autoplot()\n\n\n\n3.3 Feature extraction and\nstatistics\n{feast}에서 소개할 마지막 기능은 시계열의 feature(e.g. ACF)와\n통계량(e.g. 평균)을 뽑아내는 것입니다. {feast}에서는\nfeature() 함수를 통해 많은 종류의 features들에 대한 정보를\n제공합니다만, 본 튜토리얼에서는 시계열의 평균, 분위수, ACF를 뽑아내는\n방법에 대해서만 소개하겠습니다(see ?feature for details).\n그 외 다른 features들에 관심이 있으시다면, 여기를\n참고해주세요.\n3.3.1 Some simple statistics\n먼저, 시계열의 평균과 분위수를 뽑는 방법에 대해 소개하겠습니다. 평균,\n분위수 등 시계열의 기본적인 통계량은 feature()와 R의 기본\n함수(e.g. mean(), median())들을 이용해\n간편하게 계산할 수 있습니다. 여기서 이용할 자료 tourism()은\n지역, 주, 목적별로 나눠진 1998-2016년 분기별 호주 여행객수에 관한\n자료로, 지역, 주, 여행 목적별 여행객 수의 전체 평균과 분위수를\n계산해봤습니다:\n\n\ntourism %>%\n    features(Trips, \n             list(mean = mean, quantile))\n\n\n\n\n\n\n3.3.2 ACF features\nACF에 관한 정보는 feat_acf()를 이용하면 됩니다.\nfeat_acf()는 기본적으로 ACF와 관련한 6가지 또는 최대\n7가지의 features를 제공해줍니다(see ?feat_acf() for\ndetails):\n원 계열의 1차 자기상관계수\n원 계열의 1차-10차 자기상관계수의 제곱합\n1차 차분 계열의 1차 자기상관계수\n1차 차분 계열의 1차-10차 자기상관계수의 제곱합\n2차 차분 계열의 1차 자기상관계수\n2차 차분 계열의 1차-10차 자기상관계수의 제곱합\n(계절 시계열에 대해) 첫번째 계절 시차(seasonal lag)에서의\n자기상관계수\n\n\ntourism %>% \n  features(Trips, feat_acf)\n\n\n\n\n\n\n맨 마지막 열이 첫번째 계절 시차에서의 자기상관계수를 나타내는데, 본\n자료의 경우 분기별 자료에 해당하므로 계절 주기는 4에 해당합니다. 즉, 본\n자료에서 첫번째 계절 시차에서의 자기상관계수는 원 계열의 시차 4에서의\nACF 값을 나타낸다고 할 수 있습니다.\n\n\ntourism %>% \n  features(Trips, feat_acf) %>% \n  select(Region:Purpose, season_acf1)\n\n\n\n\n\n\n원자료에 대한 ACF를 구해보면 다음과 같이 시차 4에서의 자기상관계수와\n동일한 값을 가짐을 알 수 있죠:\n\n\ntourism %>% \n    ACF(Trips)\n\n\n\n\n\n\n본 튜토리얼에서는 소개하지 않았지만, feature()를 이용한\n시계열 feature extraction과 연계하여 다양한 시각화도 수행할 수 있습니다.\n꼭 참고해보시기 바랍니다: https://otexts.com/fpp3/stlfeatures.html\n\n4 fable\n{fable} 패키지는 tsibble 객체와 함께 tidy한 format으로 시계열 예측\n모델링을 수행할 수 있게해줍니다. {tidymodels}\n패키지에 대한 이해가 있으신 분들이라면 어렵지 않으실거라 생각합니다.\n{tidymodels}과 마찬가지로 {fable}은 여러 시계열에 대해 여러 시계열\n모형에 대한 추정, 비교, 결합, 예측 등을 가능하게해줍니다.\n본격적인 튜토리얼 시작에 앞서, tourism() 자료를 이용할\n것이며, 4가지 여행 목적(“business”, “holiday”, “visiting friends and\nrelatives”, “other reasons”)으로 분해할 수 있는 호주 멜버른(Melbourne)의\n일별 여행객 수를 예측하는 것에 관심이 있다고 가정합니다. 각 계열의 첫\n번째 관측값은 다음과 같습니다:\n\n\ntourism_melb <- tourism %>% \n  filter(Region == \"Melbourne\")\ntourism_melb %>% \n    group_by(Purpose) %>% \n    slice(1)\n\n\n\n\n\n\n우리가 추정하고자 하는 변수는 Trips(일별 여행객 수,\n단위: 천)입니다. 해당 계열들의 시도표를 보면, 추세와 약한 계절성이\n명확하게 존재함을 알 수 있습니다.\n\n\ntourism_melb %>% \n  autoplot(Trips)\n\n\n\n{fable} 패키지에서 폭넓게 쓰이는 시계열 예측 모형은 ETS와 ARIMA\n모형입니다. 먼저, ETS 모형은 추세 요소와 계절 요소를 가법, 승법,\n감쇠효과 등을 반영하여 시계열을 모델링하는 지수평활법(exponential\nsmoothing)을 통계적 모형으로 확장시킨 것에 해당합니다. 통계적 모형으로의\n확장은 오차항 \\(\\epsilon_t\\)에 대해\n통계적 분포라 할 수 있는, 평균이 0이고 분산이 \\(\\sigma^2\\)인 가우스 백색잡음 과정(gaussian\nwhite noise process)을 가정함으로써 이루어집니다. 즉, ETS 모형의 알파벳\n각각은 E(error, 오차), T(trend, 추세), S(seasonal, 계절성)을 나타내며,\n각 요소들을 모델링하는 방식(가법, 승법, 가법감쇠(damped), 승법감쇠)에\n따라 ETS 모형의 종류가 나뉘어집니다. 아울러, 각 모델은 관측된 자료를\n설명하는 측정식(measurement equations)과 시간에 따라 변화하는 관측되지\n않은 요소(level, trend, seasonal)들을 설명하는 상태식(state\nequations)으로 구성되는데, 이러한 이유에서 우리는 ETS 모형을\n혁신상태공간모형을 이루는 지수평활법(innovations state space models for\nexponential smoothing)이라고 표현하기도 합니다(See here for detail). 두 번째로,\nARIMA 모형은 시계열의 현재값을 과거값과 과거 예측 오차로 설명하는\n대표적인 통계적 시계열 예측모형으로, 자세한 설명은 생략하겠습니다. ARIMA\n모형에 대한 개념이 없으신 분들은 여기를\n참고해주시기 바랍니다.\n두 모형에 대한 간략한 개념 설명은 이쯤에서 마치기로 하고, 이제 이\n모형들을 {fable} 패키지를 이용해 어떻게 적합을 수행하면 되는지\n보겠습니다. {fable}을 이용한 모형 적합은 model()을 통해\n이루어집니다. model()을 통한 적합 과정은 {tidymodels}와\n유사하게 상당히 직관적인 이름의 함수들로 이루어집니다. 먼저,\nETS()의 경우는 R에서 일반적으로 사용하는 모형식의\nspecification를 따라서 각 요소를 반영할 수 있게 해주며, 본 예제에서는\n추세 요소만 가법적으로 설정해주고 나머지 요소는 자동으로 선택되도록\n하였습니다(AICC를 기준으로, see ?ETS for details). 그리고,\nARIMA 모형은 ARIMA() 함수로 적합할 수 있으며, 해당 함수는\n{forecast} 패키지의 auto.arima와 유사하게 default 옵션으로\nAICC 값을 기준으로 최적의 모형을 선택해 줍니다(see ?ARIMA).\nmodel()을 통해 적합이 이루어진 모형 객체는 tidy한 포맥의\n모형 테이블로 결과를 반환해줍니다. 이를 이제부터 mable(model table)\n객체라 칭하겠습니다:\n\n\nfit <- tourism_melb %>% \n  model(\n    ets = ETS(Trips ~ trend(\"A\")),\n    arima = ARIMA(Trips)\n  )\nfit\n\n\n\n\n\n\nmable 객체의 행은 각 시계열로 이루어져있으며, 열은 각 모형의\nspecification을 나타냅니다. fit이 반환하는 결과를 보면 알\n수 있듯이, 적합된 ETS 모형의 추세 요소는 모두 가법적으로 고려되었으며,\n나머지 요소들은 각 시계열에 따라서 최적의 성분이 자동으로\n선택되었습니다. ARIMA 모형 또한 AICC 값을 기준으로 한 최적의 차수들이\n반영되어 모형 적합이 잘 이루어진 것으로 보입니다. 이 mable 객체로 우리는\n모델 적합 단계에서 필요한 모든 작업을 tidy한 포맷으로 수행할 수\n있습니다.\n먼저, coef() 또는 tidy()를 통해\n모형으로부터 추정된 계수들을 추출할 수 있습니다. 아울러, 사전에\nselect() 함수를 통해 특정 모형에 대한 계수 값만을 뽑을 수도\n있습니다:\n\n\nfit %>%\n  select(Region, State, Purpose, arima) %>%\n  coef()\n\n\n\n\n\n\ntidy로 수행해도 결과는 같습니다. 다음으로\nglance()를 이용하면 모형의 적합 결과를 정보 기준(e.g. AIC,\nBIC)과 잔차의 분산 등으로 요약해줍니다.\n\n\nfit %>% \n    glance()\n\n\n\n\n\n\n만약 하나의 모형으로만 시계열 예측 모델링을 수행하고 있다면,\nreport() 함수를 이용하면 됩니다. 이는 하나의 시계열 예측\n모형의 평가를 상당히 만족스러운 포맷으로 제공해줍니다.😊 여행 목적이\n“Holiday”일 때 ETS 모형을 적합한 결과 대한 요약을\nreport()를 통해 진행해봤습니다:\n\n\nfit %>%\n    filter(Purpose == \"Holiday\") %>%\n    select(ets) %>%\n    report()\n\nSeries: Trips \nModel: ETS(M,A,A) \n  Smoothing parameters:\n    alpha = 0.03084501 \n    beta  = 0.03084499 \n    gamma = 0.0001000967 \n\n  Initial states:\n     l[0]      b[0]     s[0]    s[-1]     s[-2]    s[-3]\n 424.0777 -2.535481 -26.7441 4.256618 -10.10668 32.59417\n\n  sigma^2:  0.011\n\n      AIC      AICc       BIC \n 991.7305  994.3020 1013.1688 \n\n아울러, 모형으로부터의 적합값과 잔차는 fitted(),\nresiduals() 각각을 이용해 얻을 수 있습니다:\n\n\nfit %>%\n    fitted()\nfit %>%\n    residuals()\n\n\n\n\n\n\n\n\n적합값과 잔차를 함께 얻고 싶다면 augment()를\n사용하세요:\n\n\nfit %>% \n    augment()\n\n\n\n\n\n\n모형간 예측 정확도의 비교는 accuracy()를 이용하면\n됩니다. 여러 예측 평가 측도를 제공해줍니다:\n\n\nfit %>% \n    accuracy() %>% \n    arrange(MASE)\n\n\n\n\n\n\n참고로, 여기서는 훈련 자료(training data)에 대한 예측 성능에\n해당합니다. 본 호주 일별 여행객수에 대한 자료에서는 예측 성능 평가\n측도를 MASE로 할 경우, ETS 모형이 여행 목적이 “Other”인 경우를\n제외하고는 훨씬 더 좋은 성능을 보이고 있습니다. 향후 시점의 예측은\nforecast()로 추가적인 자료에 대한 정보 없이 바로 수행을 할\n수 있습니다:\n\n\nfc <- fit %>% \n    forecast(h = \"5 years\")\nfc\n\n\n\n\n\n\n향후 시점의 예측 결과는 fable(forecast table)로 요약되며, fable은\n예측값의 점 추정치와 예측값의 분포에 대한 정보까지 포함하여\n제공해줍니다. 예를 들어, 첫 번째 행의 시계열의 예측값의 분포는 평균이\n619, 분산이 3533인 정규분포에 해당합니다. 정규분포를 따르는 이유는, 앞서\nETS의 간략한 소개에서 설명했듯이 오차항에 대해 가우스 백색잡음 과정을\n가정했기 때문입니다. 그렇다면, 이러한 예측값의 분포에 따른 구간 추정은\n어떤 함수로 수행할 수 있을까요? 예측값의 신뢰구간은\nhilo()를 이용하면 됩니다. hilo() 함수는 fable\n객체와 함께 동작하며, 원하는 신뢰수준을 반영할 수 있게 해줍니다. 다음은\n80%, 95% 각각의 신뢰수준에 대한 구간을 추정한 것입니다:\n\n\nfc %>%\n    hilo(level = c(80, 95))\n\n\n\n\n\n\n마지막으로, 예측값에 대한 시각화는 fable 객체에 대해\nautoplot()을 적용해주면 됩니다:\n\n\nfc %>% \n  autoplot(tourism_melb)\n\n\n\n본 튜토리얼에서 소개한 함수들 외에도 {fable}의 특정 모형 객체들과\n함께 동작하는 여러 함수들이 있습니다(e.g. refit(),\ninterpolate(), components(), etc).\n튜토리얼에서 소개한 내용외에 자세한 내용이 궁금하시다면 Forecasting: Principles and\nPractices (3rd Ed.)를 참고해주세요.\n5 fable.prophet\n{fable.prophet}은 facebook에서 제안한 단일 시계열 예측모형에 대한\n적합 또한 tidy한 인터페이스로 제공해줍니다. prophet은 시계열의 시간\n종속적인 특성을 고려하는 기존의 시계열 모형(e.g. 지수평활법, ARIMA\n모형)과 달리 curve-fitting(e.g. splines)으로\n모형을 적합하며, 시계열을 다음과 같이 세 가지 요소로 분해하고 각 요소를\n시간의 함수로 가법적으로 모형화합니다.\n\\[\ny(t) = g(t) + s(t) + h(t) + \\epsilon_t\n\\]\n여기서 \\(g(t)\\)는 비주기적 변화를\n모형화하는 추세 함수, \\(s(t)\\)는 주별\n또는 연별 계절성과 같은 주기적 변화를 반영하며, \\(h(t)\\)는 불규칙하게 발생할 가능성이 있는\n휴일효과(holidays and events effects)를 모형화합니다. 세 요소 중에서도\n휴일효과에 대한 반영이 prophet의 상당히 특징적인 부분이라 할 수\n있겠으며, 모형에서 조절할 수 있는 모수들이 상당히 많아서 아주 유연하고\n디테일하게 모델링이 가능합니다. 도메인 지식이 풍부할수록 prophet을 통한\n성능 개선의 가능성은 무궁무진합니다. 본 튜토리얼에서 prophet에 대한 개념\n설명은 이쯤에서 간략하게 마치겠습니다. prophet을 이번에 처음 접하시는\n분들은 여기를\n참고해주시기 바랍니다. 개념 정리와 R을 이용한 튜토리얼 과정을\n정리해놓았는데, 여기서 소개할 tidy한 인터페이스의 이해를 위해서 꼭\n필요로 될겁니다.\n본 튜토리얼에서 prophet을 이용한 예측 모델링에 이용할 자료는 호주의\n카페, 레스토랑 및 케이터링 서비스에 관한 월 매출액 자료(단위: milions\n$AUD)입니다:\n\n\ncafe <- tsibbledata::aus_retail %>%\n    filter(Industry == \"Cafes, restaurants and catering services\")\nautoplot(cafe)\n\n\n\n주별로 나뉜 해당 자료의 각 계열은 증가하는 추세와 그에 따른 연별 계절\n패턴이 눈에 보입니다. 또한, 계절 패턴의 경우 계열의 수준(level)에\n비례하는 형태를 보이고 있으므로, 계절성을 승법적으로 고려해야할\n것입니다. 아울러, 월별 자료의 경우는 휴일 효과의 경우 계절 요소를 통해\n모형화가 가능합니다. 휴일효과에 대한 반영은 이번에 진행하지 않을\n예정입니다(기존의 prophet 인터페이스에서 수행했던 것과 같이 간단하게\n반영, see here\nfor details). 본 자료에 대해 추세 요소는 선형으로 하여(default), 연별\n계절성을 승법으로 고려하여 prophet을 적합해보았습니다:\n\n\nfit <- cafe %>%\n  model(\n    prophet = prophet(Turnover ~ season(\"year\", 4, type = \"multiplicative\"))\n  )\nfit\n\n\n\n\n\n\n각 계열에 대해 prophet이 잘 적합된 것을 확인할 수 있습니다. 적합된\n모형의 각 요소들은 components()로 추출할수 있습니다:\n\n\ncomponents(fit)\n\n\n\n\n\n\ncomponents()를 통해 주어지는 객체 자체에\nautoplot()을 수행하면 모든 요소에 대한 시각화가 한꺼번에\n가능하지만, 추세와 월별 계절 패턴에 대해서만 시각화해보겠습니다.\n\n\ncomponents(fit) %>%\n  ggplot(aes(x = Month, y = trend, colour = State)) + \n  geom_line()\n\n\n\n\n\ncomponents(fit) %>%\n  ggplot(aes(x = month(Month), y = year, \n             colour = State, group = interaction(year(Month), State))) + \n  geom_line() + \n  scale_x_continuous(breaks = 1:12, labels = month.abb) + \n  xlab(\"Month\")\n\n\n\n연별 계절패턴의 경우 주별로 대개 비슷하나, 북방 지역(the Northern\nTerritory)의 경우 다른 주들과는 크게 다른 계쩔 패턴을 보여주고 있습니다.\n마지막으로, prophet의 예측도 forecast()를 이용해 쉽게\n수행할 수 있습니다. 향후 2년에 대해 예측해보았습니다:\n\n\nfc <- fit %>% \n  forecast(h = 24)\ncafe %>% \n  ggplot(aes(x = Month, y = Turnover, colour = State)) + \n  geom_line() + \n  autolayer(fc)\n\n\n\n\nForecasting: Principles and\nPractices (3rd Ed.)에서는 prophet외에도, 벡터 자기회귀모형,\n인공신경망 기반의 시계열 예측모형, 붓스트랩 및 배깅 기법을 활용한 시계열\n예측 모형 등의 고급 시계열 예측 모형도 제공해줍니다. 관심있으신 분들은\nfpp3을 참고해보시기 바랍니다.\n맺음말\ntidyverts ecosystem이 전반적으로 작동하는 과정을 소개해 보았습니다.\n그러나, 시계열 자료의 예측 모델링 대한 이해와 더불어 tidyverts를 좀 더\n디테일하게 활용하기 위해서는, Forecasting: Principles and Practices\n(3rd Ed.)을 참고하시는게 좋을 것이라 생각합니다. tidyverse와 tidymodels를\n통해 데이터를 전처리, 예측모형 개발, 개선 등의 과정에 걸리는 시간을 크게\n단축시켰듯이, fpp3을 잘 익혀두면 시계열 예측 모델링에 전반적인 과정에\n드는 시간을 상당히 단축시킬 수 있을 겁니다.😊\n\n\n\n“Tidyverts.” n.d. https://tidyverts.org.\n\n\n\n\n",
    "preview": "posts/2022-03-11-do-time-series-analysis-with-tidyverse-principle/preview.jpg",
    "last_modified": "2022-09-19T22:33:04+09:00",
    "input_file": "tidy-tools-for-timeseries-tidyverts.knit.md"
  }
]
